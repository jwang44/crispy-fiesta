{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "First experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwang44/crispy-fiesta/blob/main/submit/First_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "518DMHJl5syD"
      },
      "source": [
        "# First experiments\n",
        "This notebook includes basic text feature design steps as given in the tutorial, and experiments with basic classifiers from sklearn. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "012IjyNqMK1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "435f41be-48db-42e1-d0b0-1dfdfa1ad9c2"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "%cd /content/drive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClhM_nfj58gm"
      },
      "source": [
        "#### Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE_hJ2-On_8X"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGy8MxtKlWxv"
      },
      "source": [
        "train = pd.read_csv('./train.csv',engine='python')\n",
        "test = pd.read_csv('./test.csv',engine='python')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHibPzaDTeVo"
      },
      "source": [
        "X_train = train.body  # train texts\n",
        "y_train = train.subreddit # train subreddits\n",
        "X_test = test.body  # test texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXCloN6pnqQr"
      },
      "source": [
        "## Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHzJTJoc5i_V"
      },
      "source": [
        "### sk-learn processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qJTvw3DMtG4"
      },
      "source": [
        "from sklearn.preprocessing import Normalizer, LabelEncoder, OneHotEncoder\r\n",
        "from sklearn.feature_extraction import text\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2vtasvRWkeM",
        "outputId": "96737e9e-802c-4fca-a912-5df072c8a6c4"
      },
      "source": [
        "# transform target labels to values\n",
        "le = LabelEncoder()\n",
        "y_train_num = le.fit_transform(y_train.values) # convert category from string to numerical (!!!!! update the variables in kcross fold)\n",
        "\n",
        "# vectorize word count\n",
        "vectorizer = CountVectorizer()\n",
        "vectors_train = vectorizer.fit_transform(X_train)\n",
        "vectors_test = vectorizer.transform(X_test)\n",
        "vectors_train = vectors_train.todense()\n",
        "vectors_test = vectors_test.todense()\n",
        "\n",
        "# onehot encoding\n",
        "onehot = OneHotEncoder(handle_unknown = 'ignore')\n",
        "vectors_train = onehot.fit_transform(vectors_train)\n",
        "vectors_test = onehot.transform(vectors_test)\n",
        "\n",
        "normalizer_train = Normalizer()\n",
        "\n",
        "# print(vectorizer.get_feature_names())\n",
        "print(vectors_train.shape)\n",
        "print(vectors_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 35729)\n",
            "(1378, 35729)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLGRP9vElqgx"
      },
      "source": [
        "#### Binary\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB-6DE9AlG9r"
      },
      "source": [
        "vectorizer = CountVectorizer(binary=True)\r\n",
        "vectors_train_binary = vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_binary = vectorizer.transform(X_test)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llzfTbCj3svy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59b22c6c-a37b-418f-d4e2-ef0f4320d709"
      },
      "source": [
        "# tf-idf\r\n",
        "tf_idf_vectorizer = TfidfVectorizer()\r\n",
        "vectors_train_idf = tf_idf_vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_idf = tf_idf_vectorizer.transform(X_test)\r\n",
        "vectors_train_idf= normalizer_train.transform(vectors_train_idf)\r\n",
        "vectors_test_idf = normalizer_train.transform(vectors_test_idf)\r\n",
        "print(vectors_train_idf.shape)\r\n",
        "print(vectors_test_idf.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 15365)\n",
            "(1378, 15365)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUx9a1yB5Ykq"
      },
      "source": [
        "### nltk processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EDORAHscEv-",
        "outputId": "96a31275-aa40-4142-92c8-a1b6fd10772d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import wordnet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL8yUINtfBzX"
      },
      "source": [
        "####Stemming\n",
        "features: `vector_train_stem`, `vector_test_stem`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7XpLsfP4qiL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55ecfd3-669c-4e2e-f486-c46118186058"
      },
      "source": [
        "# stemming\r\n",
        "class StemTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       self.wnl =PorterStemmer()\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [self.wnl.stem(t) for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "\r\n",
        "vectorizer = CountVectorizer(tokenizer=StemTokenizer())\r\n",
        "vectors_train_stem = vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_stem = vectorizer.transform(X_test)\r\n",
        "vectors_train_stem= normalizer_train.transform(vectors_train_stem)\r\n",
        "vectors_test_stem = normalizer_train.transform(vectors_test_stem)\r\n",
        "print(vectors_train_stem.shape)\r\n",
        "print(vectors_test_stem.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 8727)\n",
            "(1378, 8727)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lDtjGaCe9mt"
      },
      "source": [
        "#### Lemmatization\n",
        "features: `vector_train_Lemma`, `vector_test_Lemma`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-9TOxOl5WsA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03fc4c63-18bb-4663-916f-b9ec562101d9"
      },
      "source": [
        "# Lemmatization\r\n",
        "def get_wordnet_pos(word):\r\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\r\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\r\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\r\n",
        "                \"N\": wordnet.NOUN,\r\n",
        "                \"V\": wordnet.VERB,\r\n",
        "                \"R\": wordnet.ADV}\r\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\r\n",
        "  \r\n",
        "class New_LemmaTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       self.wnl = WordNetLemmatizer()\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [self.wnl.lemmatize(t,pos =get_wordnet_pos(t)) for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "\r\n",
        "vectorizer = CountVectorizer(tokenizer=New_LemmaTokenizer())\r\n",
        "vectors_train_Lemma = vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_Lemma = vectorizer.transform(X_test)\r\n",
        "vectors_train_Lemma= normalizer_train.transform(vectors_train_Lemma)\r\n",
        "vectors_test_Lemma = normalizer_train.transform(vectors_test_Lemma)\r\n",
        "print(vectors_train_Lemma.shape)\r\n",
        "print(vectors_test_Lemma.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 10045)\n",
            "(1378, 10045)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4ZG6yZn49rZ"
      },
      "source": [
        "#### 6 feature sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAkzJQEPuoqU"
      },
      "source": [
        "1. features: vectors_train_stop, vectors_test_stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lMh3F7M3SuX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1c8186c-0b2c-4439-ff83-3de7397a5caa"
      },
      "source": [
        "# remove stop words and punctuation, normalization\r\n",
        "stop_words = text.ENGLISH_STOP_WORDS\r\n",
        "\r\n",
        "class PuncTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       pass\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [t for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "vectorizer = CountVectorizer(stop_words = stop_words, tokenizer=PuncTokenizer())\r\n",
        "vectors_train_stop = vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_stop = vectorizer.transform(X_test)\r\n",
        "\r\n",
        "normalizer_train = Normalizer()\r\n",
        "vectors_train_stop= normalizer_train.transform(vectors_train_stop)\r\n",
        "vectors_test_stop = normalizer_train.transform(vectors_test_stop)\r\n",
        "print(vectors_train_stop.shape)\r\n",
        "print(vectors_test_stop.shape)\r\n",
        "#print(vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 12402)\n",
            "(1378, 12402)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCgoGoz2uY_o"
      },
      "source": [
        "2. features: vectors_train_stop_tfidf, vectors_test_stop_tfidf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXiGfZHJtsta",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c1f9f7-ae14-404b-f9be-922b2b583d43"
      },
      "source": [
        "# remove stop words and punctuation, tfidf, normalization\r\n",
        "stop_words = text.ENGLISH_STOP_WORDS\r\n",
        "\r\n",
        "tf_idf_transformer = TfidfTransformer()\r\n",
        "\r\n",
        "class PuncTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       pass\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [t for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "vectorizer = CountVectorizer(stop_words = stop_words, tokenizer=PuncTokenizer())\r\n",
        "vectors_train_stop_tfidf = vectorizer.fit_transform(X_train)\r\n",
        "vectors_train_stop_tfidf = tf_idf_transformer.fit_transform(vectors_train_stop_tfidf)\r\n",
        "vectors_test_stop_tfidf = vectorizer.transform(X_test)\r\n",
        "vectors_test_stop_tfidf = tf_idf_transformer.transform(vectors_test_stop_tfidf)\r\n",
        "\r\n",
        "vectors_train_stop_tfidf = normalizer_train.transform(vectors_train_stop_tfidf)\r\n",
        "vectors_test_stop_tfidf = normalizer_train.transform(vectors_test_stop_tfidf)\r\n",
        "print(vectors_train_stop_tfidf.shape)\r\n",
        "print(vectors_test_stop_tfidf.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 12402)\n",
            "(1378, 12402)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04ESAZKKvMmx"
      },
      "source": [
        "3. features: vectors_train_stop_Lemma, vectors_test_stop_Lemma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSDm_M5WvMMF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b339850-875c-4622-9eba-3fe5afa68b00"
      },
      "source": [
        "# remove stop words and punctuation, lemmatization, normalization\r\n",
        "stop_words = text.ENGLISH_STOP_WORDS\r\n",
        "\r\n",
        "def get_wordnet_pos(word):\r\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\r\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\r\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\r\n",
        "                \"N\": wordnet.NOUN,\r\n",
        "                \"V\": wordnet.VERB,\r\n",
        "                \"R\": wordnet.ADV}\r\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\r\n",
        "\r\n",
        "class New_LemmaTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       self.wnl = WordNetLemmatizer()\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [self.wnl.lemmatize(t,pos =get_wordnet_pos(t)) for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "\r\n",
        "vectorizer = CountVectorizer(stop_words = stop_words, tokenizer = New_LemmaTokenizer())\r\n",
        "vectors_train_stop_Lemma = vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_stop_Lemma = vectorizer.transform(X_test)\r\n",
        "vectors_train_stop_Lemma = normalizer_train.transform(vectors_train_stop_Lemma)\r\n",
        "vectors_test_stop_Lemma = normalizer_train.transform(vectors_test_stop_Lemma)\r\n",
        "\r\n",
        "# print(vectorizer.get_feature_names())\r\n",
        "print(vectors_train_stop_Lemma.shape)\r\n",
        "print(vectors_test_stop_Lemma.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1999, 9779)\n",
            "(1378, 9779)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVbRxM6qe5n9"
      },
      "source": [
        "4. features: `vectors_train_stop_tfidf_Lemma`, `vectors_test_stop_tfidf_Lemma`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Xyxvw6QAEP8",
        "outputId": "cd2f7df9-297f-4d0b-883e-10b27931735c"
      },
      "source": [
        "# put it all together: remove stop words and punctuation, tfidf, lemmatization, normalization\r\n",
        "stop_words = text.ENGLISH_STOP_WORDS\r\n",
        "\r\n",
        "def get_wordnet_pos(word):\r\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\r\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\r\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\r\n",
        "                \"N\": wordnet.NOUN,\r\n",
        "                \"V\": wordnet.VERB,\r\n",
        "                \"R\": wordnet.ADV}\r\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\r\n",
        "\r\n",
        "class New_LemmaTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       self.wnl = WordNetLemmatizer()\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [self.wnl.lemmatize(t,pos =get_wordnet_pos(t)) for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "\r\n",
        "tf_idf_transformer = TfidfTransformer()\r\n",
        "vectorizer = CountVectorizer(stop_words = stop_words, tokenizer = New_LemmaTokenizer())\r\n",
        "vectors_train_stop_tfidf_Lemma = vectorizer.fit_transform(X_train)\r\n",
        "vectors_train_stop_tfidf_Lemma = tf_idf_transformer.fit_transform(vectors_train_stop_tfidf_Lemma)\r\n",
        "vectors_test_stop_tfidf_Lemma = vectorizer.transform(X_test)\r\n",
        "vectors_test_stop_tfidf_Lemma = tf_idf_transformer.transform(vectors_test_stop_tfidf_Lemma)\r\n",
        "vectors_train_stop_tfidf_Lemma = normalizer_train.transform(vectors_train_stop_tfidf_Lemma)\r\n",
        "vectors_test_stop_tfidf_Lemma = normalizer_train.transform(vectors_test_stop_tfidf_Lemma)\r\n",
        "\r\n",
        "# print(vectorizer.get_feature_names())\r\n",
        "print(vectors_train_stop_tfidf_Lemma.shape)\r\n",
        "print(vectors_test_stop_tfidf_Lemma.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1999, 9779)\n",
            "(1378, 9779)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD4Jb5j9wZcO"
      },
      "source": [
        "5. features: vectors_train_stop_stem, vectors_test_stop_stem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Lt1a00rwZk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82fcc4ad-3a4e-4872-fe79-3257c9f8ada3"
      },
      "source": [
        "# remove stopwords and punctuation, stemming, normalization\r\n",
        "stop_words = text.ENGLISH_STOP_WORDS\r\n",
        "\r\n",
        "class StemTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       self.wnl =PorterStemmer()\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [self.wnl.stem(t) for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "\r\n",
        "vectorizer = CountVectorizer(stop_words = stop_words, tokenizer=StemTokenizer())\r\n",
        "vectors_train_stop_stem = vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_stop_stem = vectorizer.transform(X_test)\r\n",
        "vectors_train_stop_stem = normalizer_train.transform(vectors_train_stop_stem)\r\n",
        "vectors_test_stop_stem = normalizer_train.transform(vectors_test_stop_stem)\r\n",
        "print(vectors_train_stop_stem.shape)\r\n",
        "print(vectors_test_stop_stem.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1999, 8522)\n",
            "(1378, 8522)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OPAfLwdthTQ"
      },
      "source": [
        "6. features: vectors_train_stop_tfidf_stem, vectors_test_stop_tfidf_stem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuWWiummsCUn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b106a254-84d6-4391-9d34-34becd00487d"
      },
      "source": [
        "# remove stopwords and punctuation, tfidf, stemming, normalization\r\n",
        "stop_words = text.ENGLISH_STOP_WORDS\r\n",
        "\r\n",
        "class StemTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       self.wnl =PorterStemmer()\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [self.wnl.stem(t) for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "\r\n",
        "tf_idf_transformer = TfidfTransformer()\r\n",
        "vectorizer = CountVectorizer(stop_words = stop_words, tokenizer=StemTokenizer())\r\n",
        "vectors_train_stop_tfidf_stem = vectorizer.fit_transform(X_train)\r\n",
        "vectors_train_stop_tfidf_stem = tf_idf_transformer.fit_transform(vectors_train_stop_tfidf_stem)\r\n",
        "vectors_test_stop_tfidf_stem = vectorizer.transform(X_test)\r\n",
        "vectors_test_stop_tfidf_stem = tf_idf_transformer.transform(vectors_test_stop_tfidf_stem)\r\n",
        "vectors_train_stop_tfidf_stem = normalizer_train.transform(vectors_train_stop_tfidf_stem)\r\n",
        "vectors_test_stop_tfidf_stem = normalizer_train.transform(vectors_test_stop_tfidf_stem)\r\n",
        "print(vectors_train_stop_tfidf_stem.shape)\r\n",
        "print(vectors_test_stop_tfidf_stem.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1999, 8522)\n",
            "(1378, 8522)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJXs8wYkmbab"
      },
      "source": [
        "#### binary (for use with Bernoulli)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX0gfO5GmTre",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3efbb56-96b4-452c-f32d-5bb389780c43"
      },
      "source": [
        "# put it all together: remove stopwords, punctuation, lemmatization, \r\n",
        "stop_words = text.ENGLISH_STOP_WORDS\r\n",
        "\r\n",
        "def get_wordnet_pos(word):\r\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\r\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\r\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\r\n",
        "                \"N\": wordnet.NOUN,\r\n",
        "                \"V\": wordnet.VERB,\r\n",
        "                \"R\": wordnet.ADV}\r\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\r\n",
        "\r\n",
        "class New_LemmaTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       self.wnl = WordNetLemmatizer()\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [self.wnl.lemmatize(t,pos =get_wordnet_pos(t)) for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "\r\n",
        "vectorizer = CountVectorizer(stop_words = stop_words,tokenizer=New_LemmaTokenizer(),binary=True)\r\n",
        "vectors_train_stop_Lemma_binary = vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_stop_Lemma_binary = vectorizer.transform(X_test)\r\n",
        "\r\n",
        "\r\n",
        "# print(vectorizer.get_feature_names())\r\n",
        "# print(vectors_train_stop_Lemma_binary)\r\n",
        "print(vectors_test_stop_Lemma_binary.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1378, 9779)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymF0ztmLnbGq"
      },
      "source": [
        "## Experiments with models in sk-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XLZzvkn7oIv"
      },
      "source": [
        "# from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "\n",
        "from sklearn.model_selection import KFold, cross_val_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lifuqatv8Lv8"
      },
      "source": [
        "#### Find the best set of features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH-sQOatGy54"
      },
      "source": [
        "We have 6 different sets of features\n",
        "* vectors_train_stop, vectors_test_stop\n",
        "* vectors_train_stop_tfidf, vectors_test_stop_tfidf\n",
        "* vectors_train_stop_Lemma, vectors_test_stop_Lemma\n",
        "* vectors_train_stop_tfidf_Lemma, vectors_test_stop_tfidf_Lemma\n",
        "* vectors_train_stop_stem, vectors_test_stop_stem\n",
        "* vectors_train_stop_tfidf_stem, vectors_test_stop_tfidf_stem\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZQ1dRU4yIpa",
        "outputId": "4daea0e8-5c81-472b-d068-673342837e2c"
      },
      "source": [
        "model = LinearSVC()\n",
        "scores = cross_val_score(model, vectors_train_stop, y_train_num, cv=10)\n",
        "print(scores.mean())\n",
        "\n",
        "model = LinearSVC()\n",
        "scores = cross_val_score(model, vectors_train_stop_tfidf, y_train_num, cv=10)\n",
        "print(scores.mean())\n",
        "\n",
        "model = LinearSVC()\n",
        "scores = cross_val_score(model, vectors_train_stop_Lemma, y_train_num, cv=10)\n",
        "print(scores.mean())\n",
        "\n",
        "model = LinearSVC()\n",
        "scores = cross_val_score(model, vectors_train_stop_tfidf_Lemma, y_train_num, cv=10)\n",
        "print(scores.mean())\n",
        "\n",
        "model = LinearSVC()\n",
        "scores = cross_val_score(model, vectors_train_stop_stem, y_train_num, cv=10)\n",
        "print(scores.mean())\n",
        "\n",
        "model = LinearSVC()\n",
        "scores = cross_val_score(model, vectors_train_stop_tfidf_stem, y_train_num, cv=10)\n",
        "print(scores.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9169673366834171\n",
            "0.925469849246231\n",
            "0.9239698492462312\n",
            "0.933964824120603\n",
            "0.921467336683417\n",
            "0.9289648241206031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9ThPVgy-Gqn",
        "outputId": "6c145b11-933b-4fdd-9487-2600138bb6bd"
      },
      "source": [
        "model = LogisticRegression()\n",
        "scores = cross_val_score(model, vectors_train_stop, y_train_num, cv=10)\n",
        "print(scores.mean())\n",
        "\n",
        "model = LogisticRegression()\n",
        "scores = cross_val_score(model, vectors_train_stop_tfidf, y_train_num, cv=10)\n",
        "print(scores.mean())\n",
        "\n",
        "model = LogisticRegression()\n",
        "scores = cross_val_score(model, vectors_train_stop_Lemma, y_train_num, cv=10)\n",
        "print(scores.mean())\n",
        "\n",
        "model = LogisticRegression()\n",
        "scores = cross_val_score(model, vectors_train_stop_tfidf_Lemma, y_train_num, cv=10)\n",
        "print(scores.mean())\n",
        "\n",
        "model = LogisticRegression()\n",
        "scores = cross_val_score(model, vectors_train_stop_stem, y_train_num, cv=10)\n",
        "print(scores.mean())\n",
        "\n",
        "model = LogisticRegression()\n",
        "scores = cross_val_score(model, vectors_train_stop_tfidf_stem, y_train_num, cv=10)\n",
        "print(scores.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9014497487437186\n",
            "0.9254648241206029\n",
            "0.9069597989949749\n",
            "0.928969849246231\n",
            "0.9004572864321607\n",
            "0.927969849246231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE4PY3Pk_GcJ"
      },
      "source": [
        "#### Find the best off-the-shelf model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZxu01VN8YqP"
      },
      "source": [
        "X = vectors_train_stop_tfidf_Lemma # the best set of feature found in the previous step\n",
        "y = y_train_num"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjINFzf6xzRD",
        "outputId": "89b049c5-415a-46a0-b710-74650b6f2b97"
      },
      "source": [
        "model = LinearSVC()\n",
        "kf = KFold(n_splits=10, shuffle=True)\n",
        "train_accus = []\n",
        "test_accus = []\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    model.fit(X_train, y_train)\n",
        "    train_accus.append(model.score(X_train, y_train))\n",
        "    test_accus.append(model.score(X_test, y_test))\n",
        "train_accus = np.array(train_accus)\n",
        "test_accus = np.array(test_accus)\n",
        "print(\"-------------Linear SVC---------------\")\n",
        "print(\"train accu: \", train_accus.mean())\n",
        "print(\"test accu: \", test_accus.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------Linear SVC---------------\n",
            "train accu:  1.0\n",
            "test accu:  0.9319723618090452\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zb5ALaN3CXoJ",
        "outputId": "39193b92-f395-4163-ccd2-d7da10000d06"
      },
      "source": [
        "model = SVC()\n",
        "kf = KFold(n_splits=10, shuffle=True)\n",
        "train_accus = []\n",
        "test_accus = []\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    model.fit(X_train, y_train)\n",
        "    train_accus.append(model.score(X_train, y_train))\n",
        "    test_accus.append(model.score(X_test, y_test))\n",
        "train_accus = np.array(train_accus)\n",
        "test_accus = np.array(test_accus)\n",
        "print(\"-------------RBF SVC---------------\")\n",
        "print(\"train accu: \", train_accus.mean())\n",
        "print(\"test accu: \", test_accus.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------RBF SVC---------------\n",
            "train accu:  1.0\n",
            "test accu:  0.9234698492462311\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asU1Y_ykBxW9",
        "outputId": "69b66204-4186-4032-8128-9e61195fe5ee"
      },
      "source": [
        "model = LogisticRegression()\n",
        "kf = KFold(n_splits=10, shuffle=True)\n",
        "train_accus = []\n",
        "test_accus = []\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    model.fit(X_train, y_train)\n",
        "    train_accus.append(model.score(X_train, y_train))\n",
        "    test_accus.append(model.score(X_test, y_test))\n",
        "train_accus = np.array(train_accus)\n",
        "test_accus = np.array(test_accus)\n",
        "print(\"-------------Logistic Regression---------------\")\n",
        "print(\"train accu: \", train_accus.mean())\n",
        "print(\"test accu: \", test_accus.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------Linear SVC---------------\n",
            "train accu:  0.9909955530850473\n",
            "test accu:  0.929467336683417\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBL1StCxB3-0",
        "outputId": "19007ee5-cd57-47ef-c2e1-097f1177e8da"
      },
      "source": [
        "model = KNeighborsClassifier()\n",
        "kf = KFold(n_splits=10, shuffle=True)\n",
        "train_accus = []\n",
        "test_accus = []\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    model.fit(X_train, y_train)\n",
        "    train_accus.append(model.score(X_train, y_train))\n",
        "    test_accus.append(model.score(X_test, y_test))\n",
        "train_accus = np.array(train_accus)\n",
        "test_accus = np.array(test_accus)\n",
        "print(\"-------------K-nearest Neighbor---------------\")\n",
        "print(\"train accu: \", train_accus.mean())\n",
        "print(\"test accu: \", test_accus.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------Linear SVC---------------\n",
            "train accu:  0.9056750972762645\n",
            "test accu:  0.8489095477386934\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUqt-djHB-Ap",
        "outputId": "c7d09e0a-3659-4d97-d4da-a9b510a05051"
      },
      "source": [
        "model = DecisionTreeClassifier()\n",
        "kf = KFold(n_splits=10, shuffle=True)\n",
        "train_accus = []\n",
        "test_accus = []\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    model.fit(X_train, y_train)\n",
        "    train_accus.append(model.score(X_train, y_train))\n",
        "    test_accus.append(model.score(X_test, y_test))\n",
        "train_accus = np.array(train_accus)\n",
        "test_accus = np.array(test_accus)\n",
        "print(\"-------------Decision Tree---------------\")\n",
        "print(\"train accu: \", train_accus.mean())\n",
        "print(\"test accu: \", test_accus.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------Linear SVC---------------\n",
            "train accu:  1.0\n",
            "test accu:  0.7989145728643218\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMnaw1-7DdeO",
        "outputId": "63bf9c77-1148-4ed3-c619-4a6c2f70d16a"
      },
      "source": [
        "model = MultinomialNB()\n",
        "kf = KFold(n_splits=10, shuffle=True)\n",
        "train_accus = []\n",
        "test_accus = []\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    model.fit(X_train, y_train)\n",
        "    train_accus.append(model.score(X_train, y_train))\n",
        "    test_accus.append(model.score(X_test, y_test))\n",
        "train_accus = np.array(train_accus)\n",
        "test_accus = np.array(test_accus)\n",
        "print(\"-------------Multinomial NB---------------\")\n",
        "print(\"train accu: \", train_accus.mean())\n",
        "print(\"test accu: \", test_accus.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------Multinomial NB---------------\n",
            "train accu:  0.9808238218763512\n",
            "test accu:  0.9174748743718592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChqQWqRuD10T",
        "outputId": "eb965cf7-700c-4f1b-b826-e9fc2496cdb3"
      },
      "source": [
        "model = BernoulliNB()\n",
        "kf = KFold(n_splits=10, shuffle=True)\n",
        "train_accus = []\n",
        "test_accus = []\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    model.fit(X_train, y_train)\n",
        "    train_accus.append(model.score(X_train, y_train))\n",
        "    test_accus.append(model.score(X_test, y_test))\n",
        "train_accus = np.array(train_accus)\n",
        "test_accus = np.array(test_accus)\n",
        "print(\"-------------Bernoulli NB---------------\")\n",
        "print(\"train accu: \", train_accus.mean())\n",
        "print(\"test accu: \", test_accus.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------Bernoulli NB---------------\n",
            "train accu:  0.9438612809585573\n",
            "test accu:  0.8469170854271357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVXbMZw0EGr7"
      },
      "source": [
        "#### Grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB20XzS3FqBL"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IzciLosEMMi",
        "outputId": "b501350b-6a7c-4e0c-e8a9-59d997aa6854"
      },
      "source": [
        "model = LinearSVC()\n",
        "parameters = {\n",
        "    'C': (0.01, 0.1, 1, 10, 100, 1000)\n",
        "}\n",
        "gs_model = GridSearchCV(model, parameters, cv=10, n_jobs=-1)\n",
        "gs_model = gs_model.fit(vectors_train_stop_tfidf_Lemma, y_train_num)\n",
        "print(gs_model.best_score_)\n",
        "for param_name in sorted(parameters.keys()):\n",
        "    print(\"%s: %r\" % (param_name, gs_model.best_params_[param_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.933964824120603\n",
            "C: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIQdMYl2GGQl",
        "outputId": "0e9d302e-a43a-4de9-96f9-600890f65caf"
      },
      "source": [
        "model = SVC()\n",
        "parameters = {\n",
        "    'C': (0.01, 0.1, 1, 10, 100, 1000),\n",
        "    'gamma': (1e-3, 1e-4)\n",
        "}\n",
        "gs_model = GridSearchCV(model, parameters, cv=10, n_jobs=-1)\n",
        "gs_model = gs_model.fit(vectors_train_stop_tfidf_Lemma, y_train_num)\n",
        "print(gs_model.best_score_)\n",
        "for param_name in sorted(parameters.keys()):\n",
        "    print(\"%s: %r\" % (param_name, gs_model.best_params_[param_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9224698492462311\n",
            "C: 1000\n",
            "gamma: 0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFc-Y1GmGmqT",
        "outputId": "ce951c22-8b59-47dd-b390-3e6a0869caae"
      },
      "source": [
        "model = LogisticRegression()\n",
        "parameters = {\n",
        "    'C': (0.01, 0.1, 1, 10, 100, 1000),\n",
        "    'max_iter': (100, 1000, 5000, 10000)\n",
        "}\n",
        "gs_model = GridSearchCV(model, parameters, cv=10, n_jobs=-1)\n",
        "gs_model = gs_model.fit(vectors_train_stop_tfidf_Lemma, y_train_num)\n",
        "print(gs_model.best_score_)\n",
        "for param_name in sorted(parameters.keys()):\n",
        "    print(\"%s: %r\" % (param_name, gs_model.best_params_[param_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9344673366834171\n",
            "C: 100\n",
            "max_iter: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdTND1HmN8Bj",
        "outputId": "8a075d32-8784-4727-a9dd-a9c9fb18130f"
      },
      "source": [
        "model = KNeighborsClassifier()\n",
        "parameters = {\n",
        "    'n_neighbors': (3, 5, 10, 20, 40), \n",
        "    'p': (1, 2, 3),\n",
        "    'leaf_size': (10, 20, 30, 50)\n",
        "}\n",
        "gs_model = GridSearchCV(model, parameters, cv=10, n_jobs=-1)\n",
        "gs_model = gs_model.fit(vectors_train_stop_tfidf_Lemma, y_train_num)\n",
        "print(gs_model.best_score_)\n",
        "for param_name in sorted(parameters.keys()):\n",
        "    print(\"%s: %r\" % (param_name, gs_model.best_params_[param_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8654422110552764\n",
            "leaf_size: 10\n",
            "n_neighbors: 20\n",
            "p: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwzNMwNAP-gO",
        "outputId": "40b3805b-c4b9-4e0f-c83e-23a8b808eec7"
      },
      "source": [
        "model = DecisionTreeClassifier()\n",
        "parameters = {\n",
        "    'max_depth': (10, 100, 1000, 10000), \n",
        "    'min_samples_leaf': (1, 5, 10)\n",
        "}\n",
        "gs_model = GridSearchCV(model, parameters, cv=10, n_jobs=-1)\n",
        "gs_model = gs_model.fit(vectors_train_stop_tfidf_Lemma, y_train_num)\n",
        "print(gs_model.best_score_)\n",
        "for param_name in sorted(parameters.keys()):\n",
        "    print(\"%s: %r\" % (param_name, gs_model.best_params_[param_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7719045226130653\n",
            "max_depth: 1000\n",
            "min_samples_leaf: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZxIyqcWQC6u",
        "outputId": "0f34c164-e3a6-4ee9-859a-68e08cea613b"
      },
      "source": [
        "model = MultinomialNB()\n",
        "parameters = {\n",
        "    'alpha': (0, 0.1, 0.5, 1, 2), \n",
        "}\n",
        "gs_model = GridSearchCV(model, parameters, cv=10, n_jobs=-1)\n",
        "gs_model = gs_model.fit(vectors_train_stop_tfidf_Lemma, y_train_num)\n",
        "print(gs_model.best_score_)\n",
        "for param_name in sorted(parameters.keys()):\n",
        "    print(\"%s: %r\" % (param_name, gs_model.best_params_[param_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9254648241206029\n",
            "alpha: 0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6eUbXm6QHID",
        "outputId": "aca8fdad-d26a-4e28-81ab-06d202106f55"
      },
      "source": [
        "model = BernoulliNB()\n",
        "parameters = {\n",
        "    'alpha': (0, 0.1, 0.5, 1, 2)\n",
        "}\n",
        "gs_model = GridSearchCV(model, parameters, cv=10, n_jobs=-1)\n",
        "gs_model = gs_model.fit(vectors_train_stop_tfidf_Lemma, y_train_num)\n",
        "print(gs_model.best_score_)\n",
        "for param_name in sorted(parameters.keys()):\n",
        "    print(\"%s: %r\" % (param_name, gs_model.best_params_[param_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9034572864321608\n",
            "alpha: 0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdpWDStZEjJJ"
      },
      "source": [
        "gs_model.cv_results_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YUAKpY8TEZ1"
      },
      "source": [
        "## Make predictions on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IFZ5fY6V_Ca"
      },
      "source": [
        "model = LogisticRegression(C=100, max_iter=1000)\n",
        "model.fit(vectors_train_stop_tfidf_Lemma, y_train_num)\n",
        "y_pred = model.predict(vectors_test_stop_Lemma)\n",
        "y_pred = le.inverse_transform(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "703AVWEmjeGp"
      },
      "source": [
        "model = LinearSVC(C=1)\n",
        "model.fit(vectors_train_stop_tfidf_Lemma, y_train_num)\n",
        "y_pred = model.predict(vectors_test_stop_Lemma)\n",
        "y_pred = le.inverse_transform(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz9rCLvRkKDB"
      },
      "source": [
        "#### Write results to CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYiVSRFSiKw-"
      },
      "source": [
        "result = pd.DataFrame({'id': test.id, 'subreddit': y_pred})\n",
        "result.to_csv(\"result.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfs8FKWxWIbM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "803aae5d-d300-454b-d113-711b5a355d5b"
      },
      "source": [
        "pred_csv = pd.read_csv('result.csv',engine='python')\n",
        "pred_csv.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>subreddit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>anime</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>science</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id subreddit\n",
              "0   0   science\n",
              "1   1   science\n",
              "2   2     anime\n",
              "3   3   science\n",
              "4   4   science"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    }
  ]
}
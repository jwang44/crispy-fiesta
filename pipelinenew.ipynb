{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pipelinenew.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwang44/crispy-fiesta/blob/main/pipelinenew.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhYkxNMosYcz",
        "outputId": "a8a511af-8bb7-4e3b-e93b-829bcc099faa"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "%cd /content/drive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3YXcam7SI-H"
      },
      "source": [
        "## Load the data and get basic features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVrTh5Fdsqcg"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OMqYMZNsq_-"
      },
      "source": [
        "train = pd.read_csv('./train.csv',engine='python')\r\n",
        "test = pd.read_csv('./test.csv',engine='python')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjdGixmNss4K"
      },
      "source": [
        "X_train = train.body  # train texts\r\n",
        "y_train = train.subreddit # train subreddits\r\n",
        "X_test = test.body  # test texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95NKm9YPtJ_d"
      },
      "source": [
        "from sklearn.preprocessing import Normalizer, LabelEncoder\r\n",
        "from sklearn.feature_extraction import text\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XFoTwA_s2B-",
        "outputId": "e6461cc9-01f0-49fa-e01f-9f9d42e2bbc7"
      },
      "source": [
        "# transform target labels to values\r\n",
        "le = LabelEncoder()\r\n",
        "y_train_num = le.fit_transform(y_train.values) # convert category from string to numerical (!!!!! update the variables in kcross fold)\r\n",
        "\r\n",
        "# vectorize word count\r\n",
        "vectorizer = CountVectorizer()\r\n",
        "vectors_train = vectorizer.fit_transform(X_train)\r\n",
        "vectors_test = vectorizer.transform(X_test)\r\n",
        "\r\n",
        "normalizer_train = Normalizer()\r\n",
        "vectors_train= normalizer_train.transform(vectors_train)\r\n",
        "vectors_test= normalizer_train.transform(vectors_test)\r\n",
        "\r\n",
        "# print(vectorizer.get_feature_names())\r\n",
        "print(vectors_train.shape)\r\n",
        "print(vectors_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 15365)\n",
            "(1378, 15365)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkJsrQzH1qAD",
        "outputId": "8d862b99-8722-4bab-c1b2-e95d72cad24e"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\r\n",
        "from nltk import word_tokenize\r\n",
        "from nltk.corpus import wordnet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOD25ysB1x1o",
        "outputId": "79508b1a-5997-4e91-dfa4-e9700fc1435c"
      },
      "source": [
        "# put it all together: remove stop words and punctuation, tfidf, lemmatization, normalization\r\n",
        "stop_words = text.ENGLISH_STOP_WORDS\r\n",
        "\r\n",
        "def get_wordnet_pos(word):\r\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\r\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\r\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\r\n",
        "                \"N\": wordnet.NOUN,\r\n",
        "                \"V\": wordnet.VERB,\r\n",
        "                \"R\": wordnet.ADV}\r\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\r\n",
        "\r\n",
        "class New_LemmaTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       self.wnl = WordNetLemmatizer()\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [self.wnl.lemmatize(t,pos =get_wordnet_pos(t)) for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "\r\n",
        "tf_idf_transformer = TfidfTransformer()\r\n",
        "vectorizer = CountVectorizer(stop_words = stop_words, tokenizer = New_LemmaTokenizer(), ngram_range=(1, 2)) #unigram+bigram:ngram_range=(1, 2), only bigram:ngram_range=(2, 2)\r\n",
        "vectors_train_stop_Lemma = vectorizer.fit_transform(X_train)\r\n",
        "vectors_train_stop_tfidf_Lemma = tf_idf_transformer.fit_transform(vectors_train_stop_Lemma)\r\n",
        "vectors_test_stop_Lemma = vectorizer.transform(X_test)\r\n",
        "vectors_test_stop_tfidf_Lemma = tf_idf_transformer.transform(vectors_test_stop_Lemma)\r\n",
        "vectors_train_stop_tfidf_Lemma = normalizer_train.transform(vectors_train_stop_tfidf_Lemma)\r\n",
        "vectors_test_stop_tfidf_Lemma = normalizer_train.transform(vectors_test_stop_tfidf_Lemma)\r\n",
        "normalizer_l1 = Normalizer(norm='l1')\r\n",
        "vectors_train_stop_tfidf_l1_Lemma = normalizer_l1.transform(vectors_train_stop_tfidf_Lemma)\r\n",
        "vectors_test_stop_tfidf_l1_Lemma = normalizer_l1.transform(vectors_test_stop_tfidf_Lemma)\r\n",
        "\r\n",
        "#print(vectorizer.get_feature_names())\r\n",
        "print(vectors_train_stop_tfidf_Lemma.shape)\r\n",
        "#print(vectors_test_stop_tfidf_Lemma.shape)"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1999, 70414)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oBLHbwe13Uw",
        "outputId": "c6f087f6-f824-473a-d725-0c76213b96bf"
      },
      "source": [
        "# remove stopwords and punctuation, tfidf, stemming, normalization\r\n",
        "stop_words = text.ENGLISH_STOP_WORDS\r\n",
        "\r\n",
        "class StemTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       self.wnl =PorterStemmer()\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [self.wnl.stem(t) for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "\r\n",
        "tf_idf_transformer = TfidfTransformer()\r\n",
        "vectorizer = CountVectorizer(stop_words = stop_words, tokenizer=StemTokenizer(),ngram_range=(1, 2)) #unigram+bigram:ngram_range=(1, 2), only bigram:ngram_range=(2, 2)\r\n",
        "vectors_train_stop_tfidf_stem = vectorizer.fit_transform(X_train)\r\n",
        "vectors_train_stop_tfidf_stem = tf_idf_transformer.fit_transform(vectors_train_stop_tfidf_stem)\r\n",
        "vectors_test_stop_tfidf_stem = vectorizer.transform(X_test)\r\n",
        "vectors_test_stop_tfidf_stem = tf_idf_transformer.transform(vectors_test_stop_tfidf_stem)\r\n",
        "vectors_train_stop_tfidf_stem = normalizer_train.transform(vectors_train_stop_tfidf_stem)\r\n",
        "vectors_test_stop_tfidf_stem = normalizer_train.transform(vectors_test_stop_tfidf_stem)\r\n",
        "print(vectors_train_stop_tfidf_stem.shape)\r\n",
        "print(vectors_test_stop_tfidf_stem.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1999, 73597)\n",
            "(1378, 73597)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHTQfaQKFQLE"
      },
      "source": [
        "### ngram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9rZr3iyFBVf",
        "outputId": "7917ab9c-26f2-4521-a4d9-b466b8db3c56"
      },
      "source": [
        "pipeline = Pipeline([\r\n",
        "    ('vect', CountVectorizer(stop_words = stop_words)),\r\n",
        "    ('tfidf', TfidfTransformer()),\r\n",
        "    ('normalize',Normalizer()),\r\n",
        "    ('select', RFECV(estimator=LinearSVC(),step=2800))\r\n",
        "])\r\n",
        "\r\n",
        "parameters = {  \r\n",
        "    'vect__ngram_range': ((1,1),(1, 3), (1, 2),(2,2),(3,3)),\r\n",
        "}\r\n",
        "gs_model = GridSearchCV(pipeline, parameters, cv=10, n_jobs=-1)\r\n",
        "gs_model = gs_model.fit(X_train, y_train_num)\r\n",
        "print(gs_model.best_score_)\r\n",
        "for param_name in sorted(parameters.keys()):\r\n",
        "    print(\"%s: %r\" % (param_name, gs_model.best_params_[param_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9474773869346734\n",
            "vect__ngram_range: (1, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcDWLcpSSQUX"
      },
      "source": [
        "## 13.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfzvYQTytAXZ"
      },
      "source": [
        "from sklearn.feature_selection import SelectKBest, SelectPercentile, chi2, mutual_info_classif, f_classif, SelectFpr, SelectFwe, SelectFdr, RFE, RFECV, SelectFromModel\r\n",
        "from sklearn.ensemble import ExtraTreesClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vg5DZCdDirH"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q08-xoAUDjf1"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "from sklearn.svm import SVC, LinearSVC\r\n",
        "\r\n",
        "from sklearn.model_selection import KFold, cross_val_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyZdRcHUaKv3"
      },
      "source": [
        "### linearSVC without lemma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BprYRez7RqL8",
        "outputId": "5a861db2-3aea-4b84-dc1b-77cb781b0ff2"
      },
      "source": [
        "pipeline = Pipeline([\r\n",
        "    ('vect', CountVectorizer()),\r\n",
        "    ('tfidf', TfidfTransformer()),\r\n",
        "    ('normalize',Normalizer()),\r\n",
        "    ('select', SelectPercentile()),\r\n",
        "    ('clf', LinearSVC()),\r\n",
        "])\r\n",
        "\r\n",
        "parameters = {  \r\n",
        "    'vect__ngram_range': ((1, 1), (1, 2)),\r\n",
        "    'vect__stop_words':(None, text.ENGLISH_STOP_WORDS),\r\n",
        "    #'vect__max_features':\r\n",
        "    'tfidf__use_idf': (True, False),\r\n",
        "    'normalize__norm': ('l1','l2'),\r\n",
        "    'select__percentile': (20, 40, 60, 80, 100),\r\n",
        "    'select__score_func': (chi2, f_classif), #mutual_info_classif,\r\n",
        "    'clf__C': (0.01, 0.1, 1, 10, 100)\r\n",
        "}\r\n",
        "gs_model = GridSearchCV(pipeline, parameters, cv=10, n_jobs=-1)\r\n",
        "gs_model = gs_model.fit(X_train, y_train_num)\r\n",
        "print(gs_model.best_score_)\r\n",
        "for param_name in sorted(parameters.keys()):\r\n",
        "    print(\"%s: %r\" % (param_name, gs_model.best_params_[param_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9514824120603015\n",
            "clf__C: 10\n",
            "normalize__norm: 'l2'\n",
            "select__percentile: 100\n",
            "select__score_func: <function chi2 at 0x7f54485fc320>\n",
            "tfidf__use_idf: True\n",
            "vect__ngram_range: (1, 2)\n",
            "vect__stop_words: frozenset({'yourself', 'ltd', 'afterwards', 'only', 'already', 'everyone', 'move', 'by', 'themselves', 'someone', 'detail', 'hereafter', 'becomes', 'down', 'then', 'rather', 'thin', 'though', 'ie', 'over', 'latter', 'sometime', 'on', 'un', 'with', 'ten', 'through', 'meanwhile', 'no', 'whom', 'becoming', 'most', 'sixty', 'very', 'beforehand', 'whence', 'will', 'because', 'thereby', 'go', 'below', 'were', 'an', 'whereby', 'much', 're', 'whoever', 'always', 'sometimes', 'found', 'has', 'some', 'she', 'see', 'during', 'without', 'but', 'de', 'last', 'top', 'these', 'everything', 'others', 'mostly', 'there', 'may', 'could', 'put', 'how', 'those', 'onto', 'except', 'four', 'anyway', 'among', 'be', 'its', 'cry', 'enough', 'whatever', 'thence', 'else', 'which', 'none', 'himself', 'never', 'moreover', 'call', 'twenty', 'whose', 'herein', 'couldnt', 'whole', 'mine', 'whereafter', 'whereupon', 'nothing', 'before', 'a', 'otherwise', 'find', 'until', 'it', 'therein', 'was', 'whenever', 'under', 'empty', 'describe', 'eleven', 'nor', 'interest', 'least', 'latterly', 'fill', 'via', 'nine', 'they', 'that', 'thereupon', 'within', 'as', 'however', 'own', 'ever', 'anyhow', 'sincere', 'towards', 'please', 'throughout', 'too', 'name', 'than', 'this', 'often', 'fire', 'perhaps', 'wherever', 'the', 'amount', 'where', 'anywhere', 'hence', 'namely', 'give', 'him', 'beyond', 'can', 'once', 'yourselves', 'us', 'her', 'co', 'would', 'them', 'seems', 'mill', 'we', 'thus', 'for', 'each', 'thereafter', 'although', 'less', 'behind', 'also', 'being', 'you', 'keep', 'front', 'myself', 'seeming', 'might', 'get', 'cannot', 'amongst', 'neither', 'further', 'have', 'around', 'back', 'inc', 'whereas', 'next', 'former', 'wherein', 'anyone', 'take', 'to', 'so', 'fifteen', 'noone', 'whither', 'bottom', 'eg', 'somehow', 'two', 'other', 'another', 'upon', 'what', 'about', 'seem', 'several', 'indeed', 'one', 'off', 'fifty', 'many', 'herself', 'third', 'across', 'nobody', 'are', 'forty', 'con', 'above', 'become', 'hasnt', 'itself', 'hers', 'nevertheless', 'few', 'something', 'when', 'besides', 'five', 'therefore', 'eight', 'from', 'against', 'should', 'thru', 'well', 'if', 'more', 'amoungst', 'at', 'my', 'beside', 'been', 'hereby', 'part', 'while', 'ourselves', 'made', 'their', 'seemed', 'full', 'such', 'all', 'anything', 'now', 'our', 'everywhere', 'thick', 'hereupon', 'done', 'out', 'am', 'ours', 'into', 'whether', 'do', 'formerly', 'why', 'twelve', 'between', 'three', 'i', 'etc', 'is', 'even', 'yet', 'somewhere', 'yours', 'system', 'bill', 'hundred', 'cant', 'and', 'had', 'both', 'nowhere', 'again', 'due', 'first', 'per', 'side', 'almost', 'either', 'your', 'together', 'serious', 'six', 'still', 'after', 'along', 'or', 'must', 'any', 'toward', 'me', 'here', 'elsewhere', 'in', 'became', 'up', 'of', 'show', 'not', 'he', 'same', 'alone', 'his', 'since', 'every', 'who'})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnIEhpDZkgWJ",
        "outputId": "1d8cedea-d479-4ed8-c5fc-eac37c67984d"
      },
      "source": [
        "tf_idf_transformer = TfidfTransformer()\r\n",
        "vectorizer = CountVectorizer(stop_words = stop_words, ngram_range=(1, 2)) #unigram+bigram:ngram_range=(1, 2), only bigram:ngram_range=(2, 2)\r\n",
        "vectors_train_stop = vectorizer.fit_transform(X_train)\r\n",
        "vectors_train_stop_tfidf = tf_idf_transformer.fit_transform(vectors_train_stop)\r\n",
        "vectors_test_stop = vectorizer.transform(X_test)\r\n",
        "vectors_test_stop_tfidf = tf_idf_transformer.transform(vectors_test_stop_Lemma)\r\n",
        "vectors_train_stop_tfidf = normalizer_train.transform(vectors_train_stop_tfidf)\r\n",
        "vectors_test_stop_tfidf = normalizer_train.transform(vectors_test_stop_tfidf)\r\n",
        "\r\n",
        "select = SelectPercentile(chi2, percentile=100)\r\n",
        "vectors_train_X2_SVC = select.fit_transform(vectors_train_stop_tfidf, y_train_num)\r\n",
        "vectors_test_X2_SVC = select.transform(vectors_test_stop_tfidf)\r\n",
        "print(vectors_train_X2_SVC.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 89095)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDwDlxfXvNp9"
      },
      "source": [
        "### linearSVC with lemma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgPYf5OkDW9K",
        "outputId": "aa475175-3deb-4332-a01a-60475d439a68"
      },
      "source": [
        "pipeline = Pipeline([\r\n",
        "    ('tfidf', TfidfTransformer()),\r\n",
        "    ('normalize',Normalizer()),\r\n",
        "    ('select', SelectPercentile()),\r\n",
        "    ('clf', LinearSVC()),\r\n",
        "])\r\n",
        "\r\n",
        "parameters = {\r\n",
        "    'tfidf__use_idf': (True, False),\r\n",
        "    'normalize__norm': ('l1','l2'),\r\n",
        "    'select__percentile': (20, 40, 60, 80, 100),\r\n",
        "    'select__score_func': (chi2, f_classif),# ，mutual_info_classif\r\n",
        "    'clf__C': (0.01, 0.1, 1, 10, 100)\r\n",
        "}\r\n",
        "gs_model = GridSearchCV(pipeline, parameters, cv=10, n_jobs=-1)\r\n",
        "gs_model = gs_model.fit(vectors_train_stop_Lemma, y_train_num)\r\n",
        "print(gs_model.best_score_)\r\n",
        "for param_name in sorted(parameters.keys()):\r\n",
        "    print(\"%s: %r\" % (param_name, gs_model.best_params_[param_name]))\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.940969849246231\n",
            "clf__C: 1\n",
            "normalize__norm: 'l2'\n",
            "select__percentile: 40\n",
            "select__score_func: <function chi2 at 0x7fa8ddef2290>\n",
            "tfidf__use_idf: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKcOEtYv_-uy",
        "outputId": "fdf15e36-2a9b-4916-9f41-41f3049b428d"
      },
      "source": [
        "pipeline = Pipeline([\r\n",
        "    ('select', SelectPercentile(chi2)),\r\n",
        "    ('clf', LinearSVC()),\r\n",
        "])\r\n",
        "\r\n",
        "parameters = {\r\n",
        "    'select__percentile': (46, 50,54),\r\n",
        "    #'select__score_func': (chi2, f_classif),# ，mutual_info_classif\r\n",
        "    #'clf__C': (0.01, 0.1, 1, 10, 100)\r\n",
        "}\r\n",
        "gs_model = GridSearchCV(pipeline, parameters, cv=10, n_jobs=-1)\r\n",
        "gs_model = gs_model.fit(vectors_train_stop_tfidf_Lemma, y_train_num)\r\n",
        "print(gs_model.best_score_)\r\n",
        "for param_name in sorted(parameters.keys()):\r\n",
        "    print(\"%s: %r\" % (param_name, gs_model.best_params_[param_name]))\r\n",
        "\r\n"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9399723618090452\n",
            "select__percentile: 50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-hXoKle2fwO"
      },
      "source": [
        "pipeline = Pipeline([\r\n",
        "    ('select', SelectPercentile()),\r\n",
        "    ('clf', LinearSVC()),\r\n",
        "])\r\n",
        "\r\n",
        "parameters = {\r\n",
        "    'select__percentile': (5 ,10, 20, 40, 80),\r\n",
        "    'select__score_func': (chi2, f_classif),# ，mutual_info_classif\r\n",
        "    'clf__C': (0.01, 0.1, 1, 10, 100)\r\n",
        "}\r\n",
        "gs_model = GridSearchCV(pipeline, parameters, cv=10, n_jobs=-1)\r\n",
        "gs_model = gs_model.fit(vectors_train_stop_tfidf_Lemma, y_train_num)\r\n",
        "print(gs_model.best_score_)\r\n",
        "for param_name in sorted(parameters.keys()):\r\n",
        "    print(\"%s: %r\" % (param_name, gs_model.best_params_[param_name]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdTykd1K7EMK",
        "outputId": "06f5ddc4-5f77-48b3-fb43-0a617c0c6b4d"
      },
      "source": [
        "select = SelectPercentile(chi2, percentile=40)\r\n",
        "vectors_train_Lemma_X2_SVC = select.fit_transform(vectors_train_stop_tfidf_Lemma, y_train_num)\r\n",
        "vectors_test_Lemma_X2_SVC = select.transform(vectors_test_stop_tfidf_Lemma)\r\n",
        "print(vectors_train_Lemma_X2_SVC.shape)"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 28165)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmg8wcjx0gs4"
      },
      "source": [
        "### MultinomialNB with Lemma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qAd64HAM6NU",
        "outputId": "8994fbf9-56bc-4a97-ab41-7270cfd3259d"
      },
      "source": [
        "pipeline = Pipeline([\r\n",
        "    ('tfidf', TfidfTransformer()),\r\n",
        "    ('normalize',Normalizer()),\r\n",
        "    ('select', SelectPercentile()),\r\n",
        "    ('clf', MultinomialNB())\r\n",
        "])\r\n",
        "\r\n",
        "parameters = {   \r\n",
        "    'tfidf__use_idf': (True, False),\r\n",
        "    'normalize__norm': ('l1','l2'),\r\n",
        "    'select__percentile': (20, 40, 60, 80, 100),\r\n",
        "    'select__score_func': (chi2, f_classif),# , mutual_info_classif\r\n",
        "    'clf__alpha': (1e-10, 1e-5, 0.1, 0.5, 1, 2)\r\n",
        "}\r\n",
        "gs_model = GridSearchCV(pipeline, parameters, cv=10, n_jobs=-1)\r\n",
        "gs_model = gs_model.fit(vectors_train_stop_Lemma, y_train_num)\r\n",
        "print(gs_model.best_score_)\r\n",
        "for param_name in sorted(parameters.keys()):\r\n",
        "    print(\"%s: %r\" % (param_name, gs_model.best_params_[param_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9309723618090452\n",
            "clf__alpha: 0.1\n",
            "normalize__norm: 'l2'\n",
            "select__percentile: 60\n",
            "select__score_func: <function chi2 at 0x7fa8ddef2290>\n",
            "tfidf__use_idf: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZE46Uv9Z8Jwv",
        "outputId": "c54a2fa1-bdf9-4937-fca6-5e1e94332099"
      },
      "source": [
        "select = SelectPercentile(chi2, percentile=60)\r\n",
        "vectors_train_Lemma_X2_MNB = select.fit_transform(vectors_train_stop_tfidf_Lemma, y_train_num)\r\n",
        "vectors_test_Lemma_X2_MNB = select.transform(vectors_test_stop_tfidf_Lemma)\r\n",
        "print(vectors_train_Lemma_X2_MNB.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 42248)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJvsvSar0mVY"
      },
      "source": [
        "### BernoulliNB with lemma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3onFEHGwNuGo",
        "outputId": "5a20d913-b3cb-43a3-f0d0-5f9a8f7ac3ed"
      },
      "source": [
        "pipeline = Pipeline([\r\n",
        "    ('tfidf', TfidfTransformer()),\r\n",
        "    ('normalize',Normalizer()),\r\n",
        "    ('select', SelectPercentile()),\r\n",
        "    ('clf', BernoulliNB())\r\n",
        "])\r\n",
        "\r\n",
        "parameters = {   \r\n",
        "    'tfidf__use_idf': (True, False),\r\n",
        "    'normalize__norm': ('l1','l2'),\r\n",
        "    'select__percentile': (20, 40, 60, 80, 100),\r\n",
        "    'select__score_func': (chi2, f_classif),# , mutual_info_classif\r\n",
        "    'clf__alpha': (1e-10, 1e-5, 0.1, 0.5, 1, 2)\r\n",
        "}\r\n",
        "gs_model = GridSearchCV(pipeline, parameters, cv=10, n_jobs=-1)\r\n",
        "gs_model = gs_model.fit(vectors_train_stop_Lemma, y_train_num)\r\n",
        "print(gs_model.best_score_)\r\n",
        "for param_name in sorted(parameters.keys()):\r\n",
        "    print(\"%s: %r\" % (param_name, gs_model.best_params_[param_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9094623115577889\n",
            "clf__alpha: 1e-05\n",
            "normalize__norm: 'l2'\n",
            "select__percentile: 60\n",
            "select__score_func: <function chi2 at 0x7fa8ddef2290>\n",
            "tfidf__use_idf: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VllwVBCB8O47",
        "outputId": "e9a35f5d-36c5-484b-8064-fbcf0bb14dd3"
      },
      "source": [
        "select = SelectPercentile(chi2, percentile=60)\r\n",
        "vectors_train_Lemma_X2_BNB = select.fit_transform(vectors_train_stop_tfidf_Lemma, y_train_num)\r\n",
        "vectors_test_Lemma_X2_BNB = select.transform(vectors_test_stop_tfidf_Lemma)\r\n",
        "print(vectors_train_Lemma_X2_BNB.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 42248)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUMwkZiz5r7-"
      },
      "source": [
        "## 13.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRHfhc_uA7jv",
        "outputId": "ad1196e0-da46-4e76-9543-ac55b02c192f"
      },
      "source": [
        "pipeline = Pipeline([\r\n",
        "    ('tfidf', TfidfTransformer()),\r\n",
        "    ('normalize',Normalizer()),\r\n",
        "    ('select', RFECV(estimator=LinearSVC())),\r\n",
        "])\r\n",
        "\r\n",
        "parameters = {   \r\n",
        "    #'tfidf__use_idf': (True, False),\r\n",
        "    #'normalize__norm': ('l1','l2'),\r\n",
        "    #'select__estimator':('BernoulliNB()','LinearSVC()','MultinomialNB()'), \r\n",
        "    'select__step': (700, 1400, 2800 )\r\n",
        "}\r\n",
        "gs_model = GridSearchCV(pipeline, parameters, cv=10, n_jobs=-1)\r\n",
        "gs_model = gs_model.fit(vectors_train_stop_Lemma, y_train_num)\r\n",
        "print(gs_model.best_score_)\r\n",
        "for param_name in sorted(parameters.keys()):\r\n",
        "    print(\"%s: %r\" % (param_name, gs_model.best_params_[param_name]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9394673366834171\n",
            "select__step: 2800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LVvkPVt1eeI",
        "outputId": "7cb46990-b659-4a3a-f454-2cd991d10121"
      },
      "source": [
        "estimator = LinearSVC(C=1)\r\n",
        "select = RFECV(estimator, step=2800, scoring='accuracy')\r\n",
        "vectors_train_Lemma_RFESVC = select.fit_transform(vectors_train_stop_tfidf_Lemma, y_train_num)\r\n",
        "vectors_test_Lemma_RFESVC = select.transform(vectors_test_stop_tfidf_Lemma)\r\n",
        "print(vectors_train_Lemma_RFESVC.shape)\r\n",
        "model = LinearSVC()\r\n",
        "scores = cross_val_score(model, vectors_train_Lemma_RFESVC, y_train_num, cv=10)\r\n",
        "print(scores.mean())"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 14414)\n",
            "0.9449748743718593\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ohKtx6b6Sl2"
      },
      "source": [
        "## 13.4 L1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCHvKfmX5g0Y",
        "outputId": "7651706e-608e-4068-9608-e3ff331e9a4a"
      },
      "source": [
        "estimator = LinearSVC(C=10, penalty=\"l1\",dual=False)\r\n",
        "select = SelectFromModel(estimator)\r\n",
        "vectors_train_Lemma_SFML1 = select.fit_transform(vectors_train_stop_tfidf_Lemma, y_train_num)\r\n",
        "vectors_test_Lemma_SFML1 = select.transform(vectors_test_stop_tfidf_Lemma)\r\n",
        "print(vectors_train_Lemma_SFML1.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 2105)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD95G4ihjBZK"
      },
      "source": [
        "## TruncatedSVD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3c8GFYZ6NFqD"
      },
      "source": [
        "pipeline = Pipeline([\r\n",
        "    ('svd', TruncatedSVD()),\r\n",
        "    ('clf', LinearSVC())\r\n",
        "])\r\n",
        "\r\n",
        "parameters = {   \r\n",
        "    'svd__n_components': (150, 600, 1200, 1800),\r\n",
        "    'clf__C': (0.1, 1, 10)\r\n",
        "}\r\n",
        "gs_model = GridSearchCV(pipeline, parameters, cv=10, n_jobs=-1)\r\n",
        "gs_model = gs_model.fit(vectors_train_stop_tfidf_Lemma, y_train_num)\r\n",
        "print(gs_model.best_score_)\r\n",
        "for param_name in sorted(parameters.keys()):\r\n",
        "    print(\"%s: %r\" % (param_name, gs_model.best_params_[param_name]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf7DYm49M5w8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eaafef1-6a1e-47ee-db66-584894cce788"
      },
      "source": [
        "from sklearn.decomposition import PCA\r\n",
        "from sklearn.decomposition import TruncatedSVD\r\n",
        "#pca = PCA(n_components='mle',svd_solver='full')\r\n",
        "svd = TruncatedSVD(n_components=500)\r\n",
        "vectors_train_Lemma_svd = svd.fit_transform(vectors_train_stop_tfidf_Lemma, y_train_num)\r\n",
        "#vectors_train_Lemma_pca = pca.fit_transform(vectors_train_stop_tfidf_Lemma, y_train_num)\r\n",
        "vectors_train_Lemma_svd.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1999, 500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTF5g-lvj8Ny"
      },
      "source": [
        "## models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChysaNb76Lf3",
        "outputId": "55bc556b-bf67-468a-df74-f4ffbe92d0d1"
      },
      "source": [
        "model = LinearSVC(C=10)\r\n",
        "scores = cross_val_score(model, vectors_train_X2_SVC, y_train_num, cv=10)\r\n",
        "print(scores.mean())\r\n",
        "\r\n",
        "model = LinearSVC(C=1)\r\n",
        "scores = cross_val_score(model, vectors_train_Lemma_X2_SVC, y_train_num, cv=10)\r\n",
        "print(scores.mean())\r\n",
        "\r\n",
        "model = MultinomialNB(alpha = 0.1) \r\n",
        "scores = cross_val_score(model, vectors_train_Lemma_X2_MNB, y_train_num, cv=10)\r\n",
        "print(scores.mean())\r\n",
        "\r\n",
        "model = BernoulliNB(alpha = 0.1)\r\n",
        "scores = cross_val_score(model, vectors_train_Lemma_X2_BNB, y_train_num, cv=10)\r\n",
        "print(scores.mean())\r\n",
        "\r\n",
        "model = LinearSVC()\r\n",
        "scores = cross_val_score(model, vectors_train_Lemma_RFESVC, y_train_num, cv=10)\r\n",
        "print(scores.mean())\r\n",
        "\r\n",
        "model = LinearSVC()\r\n",
        "scores = cross_val_score(model, vectors_train_Lemma_SFML1, y_train_num, cv=10)\r\n",
        "print(scores.mean())\r\n",
        "\r\n",
        "model = LinearSVC()\r\n",
        "scores = cross_val_score(model, vectors_train_Lemma_svd, y_train_num, cv=10)\r\n",
        "print(scores.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9429748743718595\n",
            "0.9479723618090452\n",
            "0.9409773869346735\n",
            "0.9249723618090453\n",
            "0.9449748743718593\n",
            "0.9489773869346735\n",
            "0.912464824120603\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S10RrDfLjPUy",
        "outputId": "aef2442a-554f-41d6-be3a-236075e884c8"
      },
      "source": [
        "model = LinearSVC()\r\n",
        "model.fit(vectors_train_Lemma_RFESVC, y_train_num)\r\n",
        "cross_val_score(model, vectors_train_Lemma_RFESVC, y_train_num, cv=10).mean()"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9449748743718593"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk8E4JcxjQCZ"
      },
      "source": [
        "model = LinearSVC()\r\n",
        "model.fit(vectors_train_Lemma_RFESVC, y_train_num)\r\n",
        "y_pred = model.predict(vectors_test_Lemma_RFESVC)\r\n",
        "y_pred = le.inverse_transform(y_pred)"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uWn_hHp3bA6",
        "outputId": "8e291d5e-8cb6-4edc-c8a0-5fbaa9f6694c"
      },
      "source": [
        "model = LinearSVC()\r\n",
        "model.fit(vectors_train_Lemma_X2_SVC, y_train_num)\r\n",
        "cross_val_score(model, vectors_train_Lemma_X2_SVC, y_train_num, cv=10).mean()"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9479723618090452"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X3eunNo2pLl"
      },
      "source": [
        "model = LinearSVC()\r\n",
        "model.fit(vectors_train_Lemma_X2_SVC, y_train_num)\r\n",
        "y_pred = model.predict(vectors_test_Lemma_X2_SVC)\r\n",
        "y_pred = le.inverse_transform(y_pred)"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTIQyWCcHDfB",
        "outputId": "4ff53416-3bd6-4a3e-9231-99c9121cfdf4"
      },
      "source": [
        "model = LinearSVC(C=10)\r\n",
        "model.fit(vectors_train_Lemma_SFML1, y_train_num)\r\n",
        "cross_val_score(model, vectors_train_Lemma_SFML1, y_train_num, cv=10).mean()"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9574874371859297"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK55uMzdjqr_"
      },
      "source": [
        "result = pd.DataFrame({'id': test.id, 'subreddit': y_pred})\r\n",
        "result.to_csv(\"result.csv\", index=False)"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "hNQeFi6VjulR",
        "outputId": "afbad370-92be-4cbd-9ba1-e61ea8b7020b"
      },
      "source": [
        "pred_csv = pd.read_csv('result.csv',engine='python')\r\n",
        "pred_csv.head()"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>subreddit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>anime</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>science</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id subreddit\n",
              "0   0   science\n",
              "1   1   science\n",
              "2   2     anime\n",
              "3   3   science\n",
              "4   4   science"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature_binary.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwang44/crispy-fiesta/blob/main/Feature_binary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "518DMHJl5syD"
      },
      "source": [
        "# Crispy Fiesta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "012IjyNqMK1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "435f41be-48db-42e1-d0b0-1dfdfa1ad9c2"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "%cd /content/drive/MyDrive/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClhM_nfj58gm"
      },
      "source": [
        "#### Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE_hJ2-On_8X"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGy8MxtKlWxv"
      },
      "source": [
        "train = pd.read_csv('./train.csv',engine='python')\n",
        "test = pd.read_csv('./test.csv',engine='python')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHibPzaDTeVo"
      },
      "source": [
        "X_train = train.body  # train texts\n",
        "y_train = train.subreddit # train subreddits\n",
        "X_test = test.body  # test texts"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VwW2jGU524A"
      },
      "source": [
        "#### Visualize the distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PUpknna2qA7E",
        "outputId": "ffa22c3f-7dcd-4fd3-9b49-e26bd2a3cfd1"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "from  collections import  Counter\r\n",
        "a=Counter(y_train)\r\n",
        "dic = {number: value for number, value in a.items()}\r\n",
        "x = [\"Science\",\"laptop\",\"samsung\",\"tennis\",\"anime\"]\r\n",
        "y = []\r\n",
        "for i in dic.keys():\r\n",
        "  y.append(dic.get(i))\r\n",
        "for j in range(5):\r\n",
        "  y[j]=y[j]/len(y_train)\r\n",
        "df = pd.DataFrame(y, x)\r\n",
        "\r\n",
        "fig = plt.figure()\r\n",
        "ax = fig.add_axes([0,0,1,1])\r\n",
        "plt.bar(x,y,align='center')\r\n",
        "ax.set_title('Label distribution')\r\n",
        "ax.set_xlabel('Label')\r\n",
        "ax.set_ylabel('total')\r\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXCloN6pnqQr"
      },
      "source": [
        "## Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHzJTJoc5i_V"
      },
      "source": [
        "### sk-learn processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qJTvw3DMtG4"
      },
      "source": [
        "from sklearn.preprocessing import Normalizer, LabelEncoder, OneHotEncoder\r\n",
        "from sklearn.feature_extraction import text\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2vtasvRWkeM",
        "outputId": "96737e9e-802c-4fca-a912-5df072c8a6c4"
      },
      "source": [
        "# transform target labels to values\n",
        "le = LabelEncoder()\n",
        "y_train_num = le.fit_transform(y_train.values) # convert category from string to numerical (!!!!! update the variables in kcross fold)\n",
        "\n",
        "# vectorize word count\n",
        "vectorizer = CountVectorizer()\n",
        "vectors_train = vectorizer.fit_transform(X_train)\n",
        "vectors_test = vectorizer.transform(X_test)\n",
        "vectors_train = vectors_train.todense()\n",
        "vectors_test = vectors_test.todense()\n",
        "\n",
        "# onehot encoding\n",
        "onehot = OneHotEncoder(handle_unknown = 'ignore')\n",
        "vectors_train = onehot.fit_transform(vectors_train)\n",
        "vectors_test = onehot.transform(vectors_test)\n",
        "\n",
        "normalizer_train = Normalizer()\n",
        "\n",
        "# print(vectorizer.get_feature_names())\n",
        "print(vectors_train.shape)\n",
        "print(vectors_test.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 35729)\n",
            "(1378, 35729)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLGRP9vElqgx"
      },
      "source": [
        "#### Binary\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB-6DE9AlG9r"
      },
      "source": [
        "vectorizer = CountVectorizer(binary=True)\r\n",
        "vectors_train_binary = vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_binary = vectorizer.transform(X_test)\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llzfTbCj3svy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59b22c6c-a37b-418f-d4e2-ef0f4320d709"
      },
      "source": [
        "# tf-idf\r\n",
        "tf_idf_vectorizer = TfidfVectorizer()\r\n",
        "vectors_train_idf = tf_idf_vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_idf = tf_idf_vectorizer.transform(X_test)\r\n",
        "vectors_train_idf= normalizer_train.transform(vectors_train_idf)\r\n",
        "vectors_test_idf = normalizer_train.transform(vectors_test_idf)\r\n",
        "print(vectors_train_idf.shape)\r\n",
        "print(vectors_test_idf.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 15365)\n",
            "(1378, 15365)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUx9a1yB5Ykq"
      },
      "source": [
        "### nltk processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EDORAHscEv-",
        "outputId": "96a31275-aa40-4142-92c8-a1b6fd10772d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import wordnet"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL8yUINtfBzX"
      },
      "source": [
        "####Stemming\n",
        "features: `vector_train_stem`, `vector_test_stem`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7XpLsfP4qiL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55ecfd3-669c-4e2e-f486-c46118186058"
      },
      "source": [
        "# stemming\r\n",
        "class StemTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       self.wnl =PorterStemmer()\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [self.wnl.stem(t) for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "\r\n",
        "vectorizer = CountVectorizer(tokenizer=StemTokenizer())\r\n",
        "vectors_train_stem = vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_stem = vectorizer.transform(X_test)\r\n",
        "vectors_train_stem= normalizer_train.transform(vectors_train_stem)\r\n",
        "vectors_test_stem = normalizer_train.transform(vectors_test_stem)\r\n",
        "print(vectors_train_stem.shape)\r\n",
        "print(vectors_test_stem.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 8727)\n",
            "(1378, 8727)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lDtjGaCe9mt"
      },
      "source": [
        "#### Lemmatization\n",
        "features: `vector_train_Lemma`, `vector_test_Lemma`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-9TOxOl5WsA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03fc4c63-18bb-4663-916f-b9ec562101d9"
      },
      "source": [
        "# Lemmatization\r\n",
        "def get_wordnet_pos(word):\r\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\r\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\r\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\r\n",
        "                \"N\": wordnet.NOUN,\r\n",
        "                \"V\": wordnet.VERB,\r\n",
        "                \"R\": wordnet.ADV}\r\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\r\n",
        "  \r\n",
        "class New_LemmaTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       self.wnl = WordNetLemmatizer()\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [self.wnl.lemmatize(t,pos =get_wordnet_pos(t)) for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "\r\n",
        "vectorizer = CountVectorizer(tokenizer=New_LemmaTokenizer())\r\n",
        "vectors_train_Lemma = vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_Lemma = vectorizer.transform(X_test)\r\n",
        "vectors_train_Lemma= normalizer_train.transform(vectors_train_Lemma)\r\n",
        "vectors_test_Lemma = normalizer_train.transform(vectors_test_Lemma)\r\n",
        "print(vectors_train_Lemma.shape)\r\n",
        "print(vectors_test_Lemma.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 10045)\n",
            "(1378, 10045)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4ZG6yZn49rZ"
      },
      "source": [
        "#### 6 feature sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAkzJQEPuoqU"
      },
      "source": [
        "1 features: vectors_train_stop, vectors_test_stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lMh3F7M3SuX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1c8186c-0b2c-4439-ff83-3de7397a5caa"
      },
      "source": [
        "# remove stop words and punctuation, normalization\r\n",
        "stop_words = text.ENGLISH_STOP_WORDS\r\n",
        "\r\n",
        "class PuncTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       pass\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [t for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "vectorizer = CountVectorizer(stop_words = stop_words, tokenizer=PuncTokenizer())\r\n",
        "vectors_train_stop = vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_stop = vectorizer.transform(X_test)\r\n",
        "\r\n",
        "normalizer_train = Normalizer()\r\n",
        "vectors_train_stop= normalizer_train.transform(vectors_train_stop)\r\n",
        "vectors_test_stop = normalizer_train.transform(vectors_test_stop)\r\n",
        "print(vectors_train_stop.shape)\r\n",
        "print(vectors_test_stop.shape)\r\n",
        "#print(vectorizer.get_feature_names())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 12402)\n",
            "(1378, 12402)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCgoGoz2uY_o"
      },
      "source": [
        "2 features: vectors_train_stop_tfidf, vectors_test_stop_tfidf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXiGfZHJtsta",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c1f9f7-ae14-404b-f9be-922b2b583d43"
      },
      "source": [
        "# remove stop words and punctuation, tfidf, normalization\r\n",
        "stop_words = text.ENGLISH_STOP_WORDS\r\n",
        "\r\n",
        "tf_idf_transformer = TfidfTransformer()\r\n",
        "\r\n",
        "class PuncTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       pass\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [t for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "vectorizer = CountVectorizer(stop_words = stop_words, tokenizer=PuncTokenizer())\r\n",
        "vectors_train_stop_tfidf = vectorizer.fit_transform(X_train)\r\n",
        "vectors_train_stop_tfidf = tf_idf_transformer.fit_transform(vectors_train_stop_tfidf)\r\n",
        "vectors_test_stop_tfidf = vectorizer.transform(X_test)\r\n",
        "vectors_test_stop_tfidf = tf_idf_transformer.transform(vectors_test_stop_tfidf)\r\n",
        "\r\n",
        "vectors_train_stop_tfidf = normalizer_train.transform(vectors_train_stop_tfidf)\r\n",
        "vectors_test_stop_tfidf = normalizer_train.transform(vectors_test_stop_tfidf)\r\n",
        "print(vectors_train_stop_tfidf.shape)\r\n",
        "print(vectors_test_stop_tfidf.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 12402)\n",
            "(1378, 12402)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04ESAZKKvMmx"
      },
      "source": [
        "3 features: vectors_train_stop_Lemma, vectors_test_stop_Lemma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSDm_M5WvMMF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b339850-875c-4622-9eba-3fe5afa68b00"
      },
      "source": [
        "# remove stop words and punctuation, lemmatization, normalization\r\n",
        "stop_words = text.ENGLISH_STOP_WORDS\r\n",
        "\r\n",
        "def get_wordnet_pos(word):\r\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\r\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\r\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\r\n",
        "                \"N\": wordnet.NOUN,\r\n",
        "                \"V\": wordnet.VERB,\r\n",
        "                \"R\": wordnet.ADV}\r\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\r\n",
        "\r\n",
        "class New_LemmaTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       self.wnl = WordNetLemmatizer()\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [self.wnl.lemmatize(t,pos =get_wordnet_pos(t)) for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "\r\n",
        "vectorizer = CountVectorizer(stop_words = stop_words, tokenizer = New_LemmaTokenizer())\r\n",
        "vectors_train_stop_Lemma = vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_stop_Lemma = vectorizer.transform(X_test)\r\n",
        "vectors_train_stop_Lemma = normalizer_train.transform(vectors_train_stop_Lemma)\r\n",
        "vectors_test_stop_Lemma = normalizer_train.transform(vectors_test_stop_Lemma)\r\n",
        "\r\n",
        "# print(vectorizer.get_feature_names())\r\n",
        "print(vectors_train_stop_Lemma.shape)\r\n",
        "print(vectors_test_stop_Lemma.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1999, 9779)\n",
            "(1378, 9779)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVbRxM6qe5n9"
      },
      "source": [
        "4 features: `vectors_train_stop_tfidf_Lemma`, `vectors_test_stop_tfidf_Lemma`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Xyxvw6QAEP8",
        "outputId": "cd2f7df9-297f-4d0b-883e-10b27931735c"
      },
      "source": [
        "# put it all together: remove stop words and punctuation, tfidf, lemmatization, normalization\r\n",
        "stop_words = text.ENGLISH_STOP_WORDS\r\n",
        "\r\n",
        "def get_wordnet_pos(word):\r\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\r\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\r\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\r\n",
        "                \"N\": wordnet.NOUN,\r\n",
        "                \"V\": wordnet.VERB,\r\n",
        "                \"R\": wordnet.ADV}\r\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\r\n",
        "\r\n",
        "class New_LemmaTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       self.wnl = WordNetLemmatizer()\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [self.wnl.lemmatize(t,pos =get_wordnet_pos(t)) for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "\r\n",
        "tf_idf_transformer = TfidfTransformer()\r\n",
        "vectorizer = CountVectorizer(stop_words = stop_words, tokenizer = New_LemmaTokenizer())\r\n",
        "vectors_train_stop_tfidf_Lemma = vectorizer.fit_transform(X_train)\r\n",
        "vectors_train_stop_tfidf_Lemma = tf_idf_transformer.fit_transform(vectors_train_stop_tfidf_Lemma)\r\n",
        "vectors_test_stop_tfidf_Lemma = vectorizer.transform(X_test)\r\n",
        "vectors_test_stop_tfidf_Lemma = tf_idf_transformer.transform(vectors_test_stop_tfidf_Lemma)\r\n",
        "vectors_train_stop_tfidf_Lemma = normalizer_train.transform(vectors_train_stop_tfidf_Lemma)\r\n",
        "vectors_test_stop_tfidf_Lemma = normalizer_train.transform(vectors_test_stop_tfidf_Lemma)\r\n",
        "\r\n",
        "# print(vectorizer.get_feature_names())\r\n",
        "print(vectors_train_stop_tfidf_Lemma.shape)\r\n",
        "print(vectors_test_stop_tfidf_Lemma.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1999, 9779)\n",
            "(1378, 9779)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD4Jb5j9wZcO"
      },
      "source": [
        "5 features: vectors_train_stop_stem, vectors_test_stop_stem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Lt1a00rwZk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82fcc4ad-3a4e-4872-fe79-3257c9f8ada3"
      },
      "source": [
        "# remove stopwords and punctuation, stemming, normalization\r\n",
        "stop_words = text.ENGLISH_STOP_WORDS\r\n",
        "\r\n",
        "class StemTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       self.wnl =PorterStemmer()\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [self.wnl.stem(t) for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "\r\n",
        "vectorizer = CountVectorizer(stop_words = stop_words, tokenizer=StemTokenizer())\r\n",
        "vectors_train_stop_stem = vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_stop_stem = vectorizer.transform(X_test)\r\n",
        "vectors_train_stop_stem = normalizer_train.transform(vectors_train_stop_stem)\r\n",
        "vectors_test_stop_stem = normalizer_train.transform(vectors_test_stop_stem)\r\n",
        "print(vectors_train_stop_stem.shape)\r\n",
        "print(vectors_test_stop_stem.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1999, 8522)\n",
            "(1378, 8522)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OPAfLwdthTQ"
      },
      "source": [
        "6 features: vectors_train_stop_tfidf_stem, vectors_test_stop_tfidf_stem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuWWiummsCUn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b106a254-84d6-4391-9d34-34becd00487d"
      },
      "source": [
        "# remove stopwords and punctuation, tfidf, stemming, normalization\r\n",
        "stop_words = text.ENGLISH_STOP_WORDS\r\n",
        "\r\n",
        "class StemTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       self.wnl =PorterStemmer()\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [self.wnl.stem(t) for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "\r\n",
        "tf_idf_transformer = TfidfTransformer()\r\n",
        "vectorizer = CountVectorizer(stop_words = stop_words, tokenizer=StemTokenizer())\r\n",
        "vectors_train_stop_tfidf_stem = vectorizer.fit_transform(X_train)\r\n",
        "vectors_train_stop_tfidf_stem = tf_idf_transformer.fit_transform(vectors_train_stop_tfidf_stem)\r\n",
        "vectors_test_stop_tfidf_stem = vectorizer.transform(X_test)\r\n",
        "vectors_test_stop_tfidf_stem = tf_idf_transformer.transform(vectors_test_stop_tfidf_stem)\r\n",
        "vectors_train_stop_tfidf_stem = normalizer_train.transform(vectors_train_stop_tfidf_stem)\r\n",
        "vectors_test_stop_tfidf_stem = normalizer_train.transform(vectors_test_stop_tfidf_stem)\r\n",
        "print(vectors_train_stop_tfidf_stem.shape)\r\n",
        "print(vectors_test_stop_tfidf_stem.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1999, 8522)\n",
            "(1378, 8522)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJXs8wYkmbab"
      },
      "source": [
        "#### binary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX0gfO5GmTre",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3efbb56-96b4-452c-f32d-5bb389780c43"
      },
      "source": [
        "# put it all together: remove stopwords, punctuation, lemmatization, \r\n",
        "stop_words = text.ENGLISH_STOP_WORDS\r\n",
        "\r\n",
        "def get_wordnet_pos(word):\r\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\r\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\r\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\r\n",
        "                \"N\": wordnet.NOUN,\r\n",
        "                \"V\": wordnet.VERB,\r\n",
        "                \"R\": wordnet.ADV}\r\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\r\n",
        "\r\n",
        "class New_LemmaTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       self.wnl = WordNetLemmatizer()\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [self.wnl.lemmatize(t,pos =get_wordnet_pos(t)) for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "\r\n",
        "vectorizer = CountVectorizer(stop_words = stop_words,tokenizer=New_LemmaTokenizer(),binary=True)\r\n",
        "vectors_train_stop_Lemma_binary = vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_stop_Lemma_binary = vectorizer.transform(X_test)\r\n",
        "\r\n",
        "\r\n",
        "# print(vectorizer.get_feature_names())\r\n",
        "# print(vectors_train_stop_Lemma_binary)\r\n",
        "print(vectors_test_stop_Lemma_binary.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1378, 9779)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymF0ztmLnbGq"
      },
      "source": [
        "## Experiments with models in sk-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XLZzvkn7oIv"
      },
      "source": [
        "# from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "\n",
        "from sklearn.model_selection import KFold, cross_val_score"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lifuqatv8Lv8"
      },
      "source": [
        "#### Find the best set of features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH-sQOatGy54"
      },
      "source": [
        "We have 6 different sets of features\n",
        "* vectors_train_stop, vectors_test_stop\n",
        "* vectors_train_stop_tfidf, vectors_test_stop_tfidf\n",
        "* vectors_train_stop_Lemma, vectors_test_stop_Lemma\n",
        "* vectors_train_stop_tfidf_Lemma, vectors_test_stop_tfidf_Lemma\n",
        "* vectors_train_stop_stem, vectors_test_stop_stem\n",
        "* vectors_train_stop_tfidf_stem, vectors_test_stop_tfidf_stem\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZQ1dRU4yIpa",
        "outputId": "4daea0e8-5c81-472b-d068-673342837e2c"
      },
      "source": [
        "model = LinearSVC()\n",
        "scores = cross_val_score(model, vectors_train_stop, y_train_num, cv=10)\n",
        "print(scores.mean())\n",
        "\n",
        "model = LinearSVC()\n",
        "scores = cross_val_score(model, vectors_train_stop_tfidf, y_train_num, cv=10)\n",
        "print(scores.mean())\n",
        "\n",
        "model = LinearSVC()\n",
        "scores = cross_val_score(model, vectors_train_stop_Lemma, y_train_num, cv=10)\n",
        "print(scores.mean())\n",
        "\n",
        "model = LinearSVC()\n",
        "scores = cross_val_score(model, vectors_train_stop_tfidf_Lemma, y_train_num, cv=10)\n",
        "print(scores.mean())\n",
        "\n",
        "model = LinearSVC()\n",
        "scores = cross_val_score(model, vectors_train_stop_stem, y_train_num, cv=10)\n",
        "print(scores.mean())\n",
        "\n",
        "model = LinearSVC()\n",
        "scores = cross_val_score(model, vectors_train_stop_tfidf_stem, y_train_num, cv=10)\n",
        "print(scores.mean())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9169673366834171\n",
            "0.925469849246231\n",
            "0.9239698492462312\n",
            "0.933964824120603\n",
            "0.921467336683417\n",
            "0.9289648241206031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9ThPVgy-Gqn",
        "outputId": "6c145b11-933b-4fdd-9487-2600138bb6bd"
      },
      "source": [
        "model = LogisticRegression()\n",
        "scores = cross_val_score(model, vectors_train_stop, y_train_num, cv=10)\n",
        "print(scores.mean())\n",
        "\n",
        "model = LogisticRegression()\n",
        "scores = cross_val_score(model, vectors_train_stop_tfidf, y_train_num, cv=10)\n",
        "print(scores.mean())\n",
        "\n",
        "model = LogisticRegression()\n",
        "scores = cross_val_score(model, vectors_train_stop_Lemma, y_train_num, cv=10)\n",
        "print(scores.mean())\n",
        "\n",
        "model = LogisticRegression()\n",
        "scores = cross_val_score(model, vectors_train_stop_tfidf_Lemma, y_train_num, cv=10)\n",
        "print(scores.mean())\n",
        "\n",
        "model = LogisticRegression()\n",
        "scores = cross_val_score(model, vectors_train_stop_stem, y_train_num, cv=10)\n",
        "print(scores.mean())\n",
        "\n",
        "model = LogisticRegression()\n",
        "scores = cross_val_score(model, vectors_train_stop_tfidf_stem, y_train_num, cv=10)\n",
        "print(scores.mean())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9014497487437186\n",
            "0.9254648241206029\n",
            "0.9069597989949749\n",
            "0.928969849246231\n",
            "0.9004572864321607\n",
            "0.927969849246231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HTCVS2d-gDi"
      },
      "source": [
        "In the above two experiments, using LinearSVC and Logistic Regression, the best results are both achieved on the `vectors_train_stop_tfidf_Lemma` feature set. The highest 10-fold cross validation accuracy is 93.39% and 92.89% respectively. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE4PY3Pk_GcJ"
      },
      "source": [
        "#### Find the best off-the-shelf model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZxu01VN8YqP"
      },
      "source": [
        "X = vectors_train_stop_tfidf_Lemma # the best set of feature found in the previous step\n",
        "y = y_train_num"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjINFzf6xzRD",
        "outputId": "89b049c5-415a-46a0-b710-74650b6f2b97"
      },
      "source": [
        "model = LinearSVC()\n",
        "kf = KFold(n_splits=10, shuffle=True)\n",
        "train_accus = []\n",
        "test_accus = []\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    model.fit(X_train, y_train)\n",
        "    train_accus.append(model.score(X_train, y_train))\n",
        "    test_accus.append(model.score(X_test, y_test))\n",
        "train_accus = np.array(train_accus)\n",
        "test_accus = np.array(test_accus)\n",
        "print(\"-------------Linear SVC---------------\")\n",
        "print(\"train accu: \", train_accus.mean())\n",
        "print(\"test accu: \", test_accus.mean())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------Linear SVC---------------\n",
            "train accu:  1.0\n",
            "test accu:  0.9319723618090452\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb5ALaN3CXoJ",
        "outputId": "39193b92-f395-4163-ccd2-d7da10000d06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = SVC()\n",
        "kf = KFold(n_splits=10, shuffle=True)\n",
        "train_accus = []\n",
        "test_accus = []\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    model.fit(X_train, y_train)\n",
        "    train_accus.append(model.score(X_train, y_train))\n",
        "    test_accus.append(model.score(X_test, y_test))\n",
        "train_accus = np.array(train_accus)\n",
        "test_accus = np.array(test_accus)\n",
        "print(\"-------------RBF SVC---------------\")\n",
        "print(\"train accu: \", train_accus.mean())\n",
        "print(\"test accu: \", test_accus.mean())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------RBF SVC---------------\n",
            "train accu:  1.0\n",
            "test accu:  0.9234698492462311\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asU1Y_ykBxW9",
        "outputId": "69b66204-4186-4032-8128-9e61195fe5ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = LogisticRegression()\n",
        "kf = KFold(n_splits=10, shuffle=True)\n",
        "train_accus = []\n",
        "test_accus = []\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    model.fit(X_train, y_train)\n",
        "    train_accus.append(model.score(X_train, y_train))\n",
        "    test_accus.append(model.score(X_test, y_test))\n",
        "train_accus = np.array(train_accus)\n",
        "test_accus = np.array(test_accus)\n",
        "print(\"-------------Logistic Regression---------------\")\n",
        "print(\"train accu: \", train_accus.mean())\n",
        "print(\"test accu: \", test_accus.mean())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------Linear SVC---------------\n",
            "train accu:  0.9909955530850473\n",
            "test accu:  0.929467336683417\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBL1StCxB3-0",
        "outputId": "19007ee5-cd57-47ef-c2e1-097f1177e8da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = KNeighborsClassifier()\n",
        "kf = KFold(n_splits=10, shuffle=True)\n",
        "train_accus = []\n",
        "test_accus = []\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    model.fit(X_train, y_train)\n",
        "    train_accus.append(model.score(X_train, y_train))\n",
        "    test_accus.append(model.score(X_test, y_test))\n",
        "train_accus = np.array(train_accus)\n",
        "test_accus = np.array(test_accus)\n",
        "print(\"-------------K-nearest Neighbor---------------\")\n",
        "print(\"train accu: \", train_accus.mean())\n",
        "print(\"test accu: \", test_accus.mean())"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------Linear SVC---------------\n",
            "train accu:  0.9056750972762645\n",
            "test accu:  0.8489095477386934\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUqt-djHB-Ap",
        "outputId": "c7d09e0a-3659-4d97-d4da-a9b510a05051",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = DecisionTreeClassifier()\n",
        "kf = KFold(n_splits=10, shuffle=True)\n",
        "train_accus = []\n",
        "test_accus = []\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    model.fit(X_train, y_train)\n",
        "    train_accus.append(model.score(X_train, y_train))\n",
        "    test_accus.append(model.score(X_test, y_test))\n",
        "train_accus = np.array(train_accus)\n",
        "test_accus = np.array(test_accus)\n",
        "print(\"-------------Decision Tree---------------\")\n",
        "print(\"train accu: \", train_accus.mean())\n",
        "print(\"test accu: \", test_accus.mean())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------Linear SVC---------------\n",
            "train accu:  1.0\n",
            "test accu:  0.7989145728643218\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMnaw1-7DdeO",
        "outputId": "63bf9c77-1148-4ed3-c619-4a6c2f70d16a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = MultinomialNB()\n",
        "kf = KFold(n_splits=10, shuffle=True)\n",
        "train_accus = []\n",
        "test_accus = []\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    model.fit(X_train, y_train)\n",
        "    train_accus.append(model.score(X_train, y_train))\n",
        "    test_accus.append(model.score(X_test, y_test))\n",
        "train_accus = np.array(train_accus)\n",
        "test_accus = np.array(test_accus)\n",
        "print(\"-------------Multinomial NB---------------\")\n",
        "print(\"train accu: \", train_accus.mean())\n",
        "print(\"test accu: \", test_accus.mean())"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------Multinomial NB---------------\n",
            "train accu:  0.9808238218763512\n",
            "test accu:  0.9174748743718592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChqQWqRuD10T",
        "outputId": "eb965cf7-700c-4f1b-b826-e9fc2496cdb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = BernoulliNB()\n",
        "kf = KFold(n_splits=10, shuffle=True)\n",
        "train_accus = []\n",
        "test_accus = []\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    model.fit(X_train, y_train)\n",
        "    train_accus.append(model.score(X_train, y_train))\n",
        "    test_accus.append(model.score(X_test, y_test))\n",
        "train_accus = np.array(train_accus)\n",
        "test_accus = np.array(test_accus)\n",
        "print(\"-------------Bernoulli NB---------------\")\n",
        "print(\"train accu: \", train_accus.mean())\n",
        "print(\"test accu: \", test_accus.mean())"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------Bernoulli NB---------------\n",
            "train accu:  0.9438612809585573\n",
            "test accu:  0.8469170854271357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVXbMZw0EGr7"
      },
      "source": [
        "#### Grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB20XzS3FqBL"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IzciLosEMMi",
        "outputId": "b787a830-5153-49df-b21a-9cbf9598be60"
      },
      "source": [
        "model = LinearSVC()\n",
        "parameters = {\n",
        "    'C': (0.01, 0.1, 1, 10)\n",
        "}\n",
        "gs_model = GridSearchCV(model, parameters, cv=10, n_jobs=-1)\n",
        "gs_model = gs_model.fit(vectors_train_stop_tfidf_Lemma, y_train_num)\n",
        "print(gs_model.best_score_)\n",
        "for param_name in sorted(parameters.keys()):\n",
        "    print(\"%s: %r\" % (param_name, gs_model.best_params_[param_name]))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.933964824120603\n",
            "C: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIQdMYl2GGQl",
        "outputId": "1d65accd-1fbd-4e91-9096-d5c8efd8a62a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = SVC()\n",
        "parameters = {\n",
        "    'C': (0.01, 0.1, 1, 10)\n",
        "}\n",
        "gs_model = GridSearchCV(model, parameters, cv=10, n_jobs=-1)\n",
        "gs_model = gs_model.fit(vectors_train_stop_tfidf_Lemma, y_train_num)\n",
        "print(gs_model.best_score_)\n",
        "for param_name in sorted(parameters.keys()):\n",
        "    print(\"%s: %r\" % (param_name, gs_model.best_params_[param_name]))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9299673366834172\n",
            "C: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFc-Y1GmGmqT",
        "outputId": "36c3d747-4a35-41f8-a30b-190f7016a582",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = LogisticRegression()\n",
        "parameters = {\n",
        "    'C': (0.01, 0.1, 1, 10),\n",
        "    'max_iter': (100, 1000, 5000, 10000)\n",
        "}\n",
        "gs_model = GridSearchCV(model, parameters, cv=10, n_jobs=-1)\n",
        "gs_model = gs_model.fit(vectors_train_stop_tfidf_Lemma, y_train_num)\n",
        "print(gs_model.best_score_)\n",
        "for param_name in sorted(parameters.keys()):\n",
        "    print(\"%s: %r\" % (param_name, gs_model.best_params_[param_name]))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9334748743718592\n",
            "C: 10\n",
            "max_iter: 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdpWDStZEjJJ"
      },
      "source": [
        "gs_model.cv_results_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guvT3RTzGgxk"
      },
      "source": [
        "## Bernoulli NB model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUmRiQt-oF5e"
      },
      "source": [
        "import time\n",
        "import random\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMSndDpV918Z"
      },
      "source": [
        "class Bernoulli_NB():\r\n",
        "  def __init__(self, LaplaceSmoothing = True):\r\n",
        "    self.LaplaceSmoothing = LaplaceSmoothing\r\n",
        "    self.Prob_Y = None      # P(Y)\r\n",
        "    self.Prob_X_Y = None     # P(xj|Y)\r\n",
        "    self.Prob_X_Y = None     # P(xj|Y)\r\n",
        "    self.n_class = 0\r\n",
        "    self.w0 = None\r\n",
        "    self.w = None\r\n",
        "    self.Ytarget = None\r\n",
        "\r\n",
        "  def ProbY(self, Y):\r\n",
        "    # calculate P(Y=1) and P(Y=0)\r\n",
        "    ProbY = np.zeros((1,2))\r\n",
        "    ProbY[0,1] = np.sum(Y)/np.shape(Y)\r\n",
        "    ProbY[0,0] = 1 - ProbY[0,1]\r\n",
        "    return ProbY\r\n",
        "\r\n",
        "  def ProbX_Yi(self, X, Y, label):\r\n",
        "    # calculte P(xj=1|Yi=1)\r\n",
        "    rows,cols = X.shape       # feature shape\r\n",
        "    numerator = np.zeros((1,cols))    # initialize numerator\r\n",
        "\r\n",
        "    # xj=1 and Yi=1\r\n",
        "    for n in range(rows):\r\n",
        "      if Y[n] == label:\r\n",
        "        numerator += X[n,:]\r\n",
        "    # Yi=1\r\n",
        "    denominator = np.count_nonzero(Y == label)\r\n",
        "\r\n",
        "    # Laplace Smoothing\r\n",
        "    if(self.LaplaceSmoothing):\r\n",
        "      numerator += 1\r\n",
        "      denominator += 2\r\n",
        "\r\n",
        "    # P(xj=1|Yi=1)\r\n",
        "    prob = numerator/denominator\r\n",
        "    return prob    \r\n",
        "\r\n",
        "  def fit(self, X, Y):\r\n",
        "    print('---------------------- start fitting ---------------------')\r\n",
        "    t1 = time.time()\r\n",
        "    \r\n",
        "    rows,cols = X.shape              # feature shape\r\n",
        "    self.n_class = len(np.unique(Y))       # number of classes\r\n",
        "    self.Prob_Y = np.zeros((self.n_class,2))   # initialize P(Y)\r\n",
        "    self.Prob_X_Y = np.zeros((self.n_class,2,cols)) # initialize P(x|Y)\r\n",
        "    c = np.zeros((self.n_class,cols))       # rows:class cols:xj\r\n",
        "    d = np.zeros((self.n_class,cols))       # rows:class cols:xj\r\n",
        "    self.w0 = np.zeros((1,self.n_class))     # [w0Y1,w0Y2,...]\r\n",
        "    self.w = np.zeros((self.n_class,cols))    # [(w1,w2,...)Y1;\r\n",
        "                            # (w1,w2,...)Y2]\r\n",
        "    for Yi in range(self.n_class):\r\n",
        "      Y_onevsall = np.where(Y == Yi, 1, 0)    # only have 2 classes: Yi(1) & notYi(0)\r\n",
        "      self.Prob_Y[Yi,:] = self.ProbY(Y_onevsall)   # [P(notYi), P(Yi)]\r\n",
        "      self.Prob_X_Y[Yi,0,:] = self.ProbX_Yi(X,Y_onevsall,0)  # [P(x1|notYi), P(x2|notYi),...]\r\n",
        "      self.Prob_X_Y[Yi,1,:] = self.ProbX_Yi(X,Y_onevsall,1)  # [P(x1|Yi), P(x2|Yi),...]\r\n",
        "      c[Yi,:] = np.log10(self.Prob_X_Y[Yi,1,:]/self.Prob_X_Y[Yi,0,:])     # log(P(xj|Y=1)/P(xj|Y=0))\r\n",
        "      d[Yi,:] = np.log10((1-self.Prob_X_Y[Yi,1,:])/(1-self.Prob_X_Y[Yi,0,:])) # log((1-P(xj|Y=1))/(1-P(xj|Y=0)))\r\n",
        "      self.w0[0,Yi] = np.log10(self.Prob_Y[Yi,1]/self.Prob_Y[Yi,0]) + np.sum(d[Yi,:])\r\n",
        "      self.w[Yi,:] = c[Yi,:] - d[Yi,:]\r\n",
        "\r\n",
        "    print('------ fit done, total time: ',time.time()-t1,' -----')\r\n",
        "    # return self.Prob_Y,self.Prob_X_Y\r\n",
        "\r\n",
        "  def predict(self, X):\r\n",
        "    print('---------------------- start predict ---------------------')\r\n",
        "    t1 = time.time()\r\n",
        "    \r\n",
        "    #X = self.encoder(X)\r\n",
        "    rows,cols = np.shape(X)       # feature shape\r\n",
        "    PreY = np.zeros(rows) # initialize Y\r\n",
        "    # print('type',type(PreY),'pre',PreY)\r\n",
        "    LogOddsRatio = np.zeros((1,self.n_class))  # initialize log odds ratio a(x) \r\n",
        "    Logistic = np.zeros((1,self.n_class))    # initialize logistic function\r\n",
        "    Y_index = 0\r\n",
        "    for obs in range(rows):\r\n",
        "      for Yi in range(self.n_class): \r\n",
        "        part2 = self.w[Yi,:].reshape(1,cols) @ np.transpose(X[obs,:])\r\n",
        "        LogOddsRatio[0,Yi] = self.w0[0,Yi] + part2.astype(np.float64)\r\n",
        "        Logistic[0,Yi] = 1/(1+np.exp(-LogOddsRatio[0,Yi]))\r\n",
        "\r\n",
        "      pre = np.where(Logistic == np.amax(Logistic))[1]    # find max logistic (type:numpy.ndarray)\r\n",
        "      # random pick one if have multiple max logistic\r\n",
        "      if pre.shape[0] > 1:\r\n",
        "        PreY[obs] = random.choice(pre)\r\n",
        "      else:\r\n",
        "        PreY[obs] = pre\r\n",
        "    print('------ predict done, total time: ',time.time()-t1,' -----')\r\n",
        "    return PreY\r\n",
        "\r\n",
        "  def score(self,X,Y):\r\n",
        "    # Return the mean accuracy on the given test data and labels.\r\n",
        "    PreY = self.predict(X)\r\n",
        "    rows = np.shape(Y)[0]\r\n",
        "    n_correct = 0;\r\n",
        "    for obs in range(rows):\r\n",
        "      if PreY[obs] == Y[obs]:\r\n",
        "        n_correct += 1\r\n",
        "    #print('n:',n_correct)\r\n",
        "    #print('rows:',rows)\r\n",
        "    accuracy = n_correct / rows\r\n",
        "    print('------------------ accuracy:',accuracy,' -----------------')\r\n",
        "    return accuracy\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfnCeH3hCxtV"
      },
      "source": [
        "B = Bernoulli_NB()\r\n",
        "'''\r\n",
        "parameters = {\r\n",
        "...     'vect__ngram_range': [(1, 1), (1, 2)],\r\n",
        "...     'tfidf__use_idf': (True, False),\r\n",
        "... }'''\r\n",
        "\r\n",
        "B.fit(vectors_train_stop_Lemma_binary,y_train_num)\r\n",
        "B.score(vectors_train_stop_Lemma_binary,y_train_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NEjfLwp4wJi"
      },
      "source": [
        "## Kfold NB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3FUZTPX4wJu"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "model = Bernoulli_NB()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuIxZX3z4wJv"
      },
      "source": [
        "X = vectors_train_stop_Lemma_binary\n",
        "y = y_train_num\n",
        "# X_test = vectors_test_stop_Lemma\n",
        "# y_test unknown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcrmPUCE4wJw",
        "outputId": "d9e864c1-e77c-404c-9171-728b572f1028"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=10, shuffle=True)\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"test accu: \", model.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------- start fitting ---------------------\n",
            "------ fit done, total time:  1.328420639038086  -----\n",
            "---------------------- start predict ---------------------\n",
            "------ predict done, total time:  0.21991395950317383  -----\n",
            "------------------ accuracy: 0.845  -----------------\n",
            "test accu:  0.845\n",
            "---------------------- start fitting ---------------------\n",
            "------ fit done, total time:  1.2664461135864258  -----\n",
            "---------------------- start predict ---------------------\n",
            "------ predict done, total time:  0.22065520286560059  -----\n",
            "------------------ accuracy: 0.88  -----------------\n",
            "test accu:  0.88\n",
            "---------------------- start fitting ---------------------\n",
            "------ fit done, total time:  1.268765926361084  -----\n",
            "---------------------- start predict ---------------------\n",
            "------ predict done, total time:  0.22852420806884766  -----\n",
            "------------------ accuracy: 0.89  -----------------\n",
            "test accu:  0.89\n",
            "---------------------- start fitting ---------------------\n",
            "------ fit done, total time:  1.324357032775879  -----\n",
            "---------------------- start predict ---------------------\n",
            "------ predict done, total time:  0.31879639625549316  -----\n",
            "------------------ accuracy: 0.885  -----------------\n",
            "test accu:  0.885\n",
            "---------------------- start fitting ---------------------\n",
            "------ fit done, total time:  1.280158281326294  -----\n",
            "---------------------- start predict ---------------------\n",
            "------ predict done, total time:  0.2823038101196289  -----\n",
            "------------------ accuracy: 0.88  -----------------\n",
            "test accu:  0.88\n",
            "---------------------- start fitting ---------------------\n",
            "------ fit done, total time:  1.3073244094848633  -----\n",
            "---------------------- start predict ---------------------\n",
            "------ predict done, total time:  0.29816460609436035  -----\n",
            "------------------ accuracy: 0.895  -----------------\n",
            "test accu:  0.895\n",
            "---------------------- start fitting ---------------------\n",
            "------ fit done, total time:  1.3082973957061768  -----\n",
            "---------------------- start predict ---------------------\n",
            "------ predict done, total time:  0.2751460075378418  -----\n",
            "------------------ accuracy: 0.905  -----------------\n",
            "test accu:  0.905\n",
            "---------------------- start fitting ---------------------\n",
            "------ fit done, total time:  1.2778041362762451  -----\n",
            "---------------------- start predict ---------------------\n",
            "------ predict done, total time:  0.3201777935028076  -----\n",
            "------------------ accuracy: 0.89  -----------------\n",
            "test accu:  0.89\n",
            "---------------------- start fitting ---------------------\n",
            "------ fit done, total time:  1.3734817504882812  -----\n",
            "---------------------- start predict ---------------------\n",
            "------ predict done, total time:  0.25539469718933105  -----\n",
            "------------------ accuracy: 0.88  -----------------\n",
            "test accu:  0.88\n",
            "---------------------- start fitting ---------------------\n",
            "------ fit done, total time:  1.3048720359802246  -----\n",
            "---------------------- start predict ---------------------\n",
            "------ predict done, total time:  0.3095710277557373  -----\n",
            "------------------ accuracy: 0.9045226130653267  -----------------\n",
            "test accu:  0.9045226130653267\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
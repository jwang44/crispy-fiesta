{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature_binary.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwang44/crispy-fiesta/blob/main/Feature_binary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "012IjyNqMK1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "492dba18-0c04-415b-9587-d88c4fb40d21"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "%cd /content/drive/MyDrive/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE_hJ2-On_8X"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from  collections import  Counter"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGy8MxtKlWxv"
      },
      "source": [
        "train = pd.read_csv('./train.csv',engine='python')\n",
        "test = pd.read_csv('./test.csv',engine='python')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHibPzaDTeVo"
      },
      "source": [
        "X_train = train.body  # train texts\n",
        "y_train = train.subreddit # train subreddits\n",
        "X_test = test.body  # test texts"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "PUpknna2qA7E",
        "outputId": "838593a2-2865-4cbb-c218-b9b8298b7830"
      },
      "source": [
        "a=Counter(y_train)\r\n",
        "dic = {number: value for number, value in a.items()}\r\n",
        "x = [\"Science\",\"laptop\",\"samsung\",\"tennis\",\"anime\"]\r\n",
        "y = []\r\n",
        "for i in dic.keys():\r\n",
        "  y.append(dic.get(i))\r\n",
        "for j in range(5):\r\n",
        "  y[j]=y[j]/len(y_train)\r\n",
        "df = pd.DataFrame(y, x)\r\n",
        "\r\n",
        "plt.bar(x,y,align='center')\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXCloN6pnqQr"
      },
      "source": [
        "## Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj2WEwNHdAfM"
      },
      "source": [
        "###Processing based on sk-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qJTvw3DMtG4"
      },
      "source": [
        "from sklearn.preprocessing import Normalizer, LabelEncoder, OneHotEncoder\r\n",
        "from sklearn.feature_extraction import text\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2vtasvRWkeM",
        "outputId": "9a8fc433-189a-4655-ceeb-fb8f46d7cd4e"
      },
      "source": [
        "# transform target labels to values\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train.values)\n",
        "\n",
        "# vectorize word count\n",
        "vectorizer = CountVectorizer()\n",
        "vectors_train = vectorizer.fit_transform(X_train)\n",
        "vectors_test = vectorizer.transform(X_test)\n",
        "vectors_train = vectors_train.todense()\n",
        "vectors_test = vectors_test.todense()\n",
        "\n",
        "# onehot encoding\n",
        "onehot = OneHotEncoder(handle_unknown = 'ignore')\n",
        "vectors_train = onehot.fit_transform(vectors_train)\n",
        "vectors_test = onehot.transform(vectors_test)\n",
        "\n",
        "# print(vectorizer.get_feature_names())\n",
        "print(vectors_train.shape)\n",
        "print(vectors_test.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 35729)\n",
            "(1378, 35729)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLGRP9vElqgx"
      },
      "source": [
        "# **Binary**\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB-6DE9AlG9r"
      },
      "source": [
        "vectorizer = CountVectorizer(binary=True)\r\n",
        "vectors_train_binary = vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_binary = vectorizer.transform(X_test)\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lMh3F7M3SuX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aab8d96-0043-4053-c84f-9e969e3d5c11"
      },
      "source": [
        "# remove stop words\r\n",
        "stop_words = text.ENGLISH_STOP_WORDS\r\n",
        "vectorizer = CountVectorizer(stop_words = stop_words)\r\n",
        "vectors_train_stop = vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_stop = vectorizer.transform(X_test)\r\n",
        "\r\n",
        "normalizer_train = Normalizer()\r\n",
        "vectors_train_stop= normalizer_train.transform(vectors_train_stop)\r\n",
        "vectors_test_stop = normalizer_train.transform(vectors_test_stop)\r\n",
        "print(vectors_train_stop.shape)\r\n",
        "print(vectors_test_stop.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 15079)\n",
            "(1378, 15079)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llzfTbCj3svy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04a7a5a9-c409-43c4-9b3e-07e5ded9a43a"
      },
      "source": [
        "# tf-idf\r\n",
        "tf_idf_vectorizer = TfidfVectorizer()\r\n",
        "vectors_train_idf = tf_idf_vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_idf = tf_idf_vectorizer.transform(X_test)\r\n",
        "vectors_train_idf= normalizer_train.transform(vectors_train_idf)\r\n",
        "vectors_test_idf = normalizer_train.transform(vectors_test_idf)\r\n",
        "print(vectors_train_idf.shape)\r\n",
        "print(vectors_test_idf.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 15365)\n",
            "(1378, 15365)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z915xw1Hc6s9"
      },
      "source": [
        "### Processing based on nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EDORAHscEv-",
        "outputId": "1b05525b-c920-45e1-cbe5-795af8e0f001"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import wordnet"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL8yUINtfBzX"
      },
      "source": [
        "####Stemming\n",
        "features: `vector_train_stem`, `vector_test_stem`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7XpLsfP4qiL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd67f7e3-966d-4d41-b28b-8d6eb5a41213"
      },
      "source": [
        "# stemming\r\n",
        "class StemTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       self.wnl =PorterStemmer()\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [self.wnl.stem(t) for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "\r\n",
        "vectorizer = CountVectorizer(tokenizer=StemTokenizer())\r\n",
        "vectors_train_stem = vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_stem = vectorizer.transform(X_test)\r\n",
        "vectors_train_stem= normalizer_train.transform(vectors_train_stem)\r\n",
        "vectors_test_stem = normalizer_train.transform(vectors_test_stem)\r\n",
        "print(vectors_train_stem.shape)\r\n",
        "print(vectors_test_stem.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 8727)\n",
            "(1378, 8727)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lDtjGaCe9mt"
      },
      "source": [
        "#### Lemmatization\n",
        "features: `vector_train_Lemma`, `vector_test_Lemma`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-9TOxOl5WsA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d58d58-bd01-4f90-8eeb-f0603b2a1686"
      },
      "source": [
        "# Lemmatization\r\n",
        "def get_wordnet_pos(word):\r\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\r\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\r\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\r\n",
        "                \"N\": wordnet.NOUN,\r\n",
        "                \"V\": wordnet.VERB,\r\n",
        "                \"R\": wordnet.ADV}\r\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\r\n",
        "  \r\n",
        "class New_LemmaTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       self.wnl = WordNetLemmatizer()\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [self.wnl.lemmatize(t,pos =get_wordnet_pos(t)) for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "\r\n",
        "vectorizer = CountVectorizer(tokenizer=New_LemmaTokenizer())\r\n",
        "vectors_train_Lemma = vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_Lemma = vectorizer.transform(X_test)\r\n",
        "vectors_train_Lemma= normalizer_train.transform(vectors_train_Lemma)\r\n",
        "vectors_test_Lemma = normalizer_train.transform(vectors_test_Lemma)\r\n",
        "print(vectors_train_Lemma.shape)\r\n",
        "print(vectors_test_Lemma.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 10045)\n",
            "(1378, 10045)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVbRxM6qe5n9"
      },
      "source": [
        "#### Put it all together\n",
        "features: `vector_train_stop_Lemma`, `vector_test_stop_Lemma`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Xyxvw6QAEP8",
        "outputId": "172a803a-a5ec-43f8-d4a1-a0555b1f59c9"
      },
      "source": [
        "# put it all together: remove stopwords, tfidf, punctuation, lemmatization, normalization\r\n",
        "stop_words = text.ENGLISH_STOP_WORDS\r\n",
        "\r\n",
        "def get_wordnet_pos(word):\r\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\r\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\r\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\r\n",
        "                \"N\": wordnet.NOUN,\r\n",
        "                \"V\": wordnet.VERB,\r\n",
        "                \"R\": wordnet.ADV}\r\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\r\n",
        "\r\n",
        "class New_LemmaTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       self.wnl = WordNetLemmatizer()\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [self.wnl.lemmatize(t,pos =get_wordnet_pos(t)) for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "\r\n",
        "tf_idf_transformer = TfidfTransformer()\r\n",
        "vectorizer = CountVectorizer(stop_words = stop_words,tokenizer=New_LemmaTokenizer())\r\n",
        "vectors_train_stop_Lemma = vectorizer.fit_transform(X_train)\r\n",
        "vectors_train_stop_Lemma = tf_idf_transformer.fit_transform(vectors_train_stop_Lemma)\r\n",
        "\r\n",
        "vectors_test_stop_Lemma = vectorizer.transform(X_test)\r\n",
        "vectors_test_stop_Lemma = tf_idf_transformer.transform(vectors_test_stop_Lemma)\r\n",
        "vectors_train_stop_Lemma = normalizer_train.transform(vectors_train_stop_Lemma)\r\n",
        "vectors_test_stop_Lemma = normalizer_train.transform(vectors_test_stop_Lemma)\r\n",
        "\r\n",
        "# print(vectorizer.get_feature_names())\r\n",
        "print(vectors_train_stop_Lemma.shape)\r\n",
        "print(vectors_test_stop_Lemma.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1999, 9779)\n",
            "(1378, 9779)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJXs8wYkmbab"
      },
      "source": [
        "# **put it all together binary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX0gfO5GmTre",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5b7b4eb-bd08-440b-afa1-8bb571bc9897"
      },
      "source": [
        "# put it all together: remove stopwords, punctuation, lemmatization, \r\n",
        "stop_words = text.ENGLISH_STOP_WORDS\r\n",
        "\r\n",
        "def get_wordnet_pos(word):\r\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\r\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\r\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\r\n",
        "                \"N\": wordnet.NOUN,\r\n",
        "                \"V\": wordnet.VERB,\r\n",
        "                \"R\": wordnet.ADV}\r\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\r\n",
        "\r\n",
        "class New_LemmaTokenizer:\r\n",
        "     def __init__(self):\r\n",
        "       self.wnl = WordNetLemmatizer()\r\n",
        "     def __call__(self, doc):\r\n",
        "       return [self.wnl.lemmatize(t,pos =get_wordnet_pos(t)) for t in word_tokenize(doc) if t.isalpha()]\r\n",
        "\r\n",
        "vectorizer = CountVectorizer(stop_words = stop_words,tokenizer=New_LemmaTokenizer(),binary=True)\r\n",
        "vectors_train_stop_Lemma_binary = vectorizer.fit_transform(X_train)\r\n",
        "vectors_test_stop_Lemma_binary = vectorizer.transform(X_test)\r\n",
        "\r\n",
        "\r\n",
        "# print(vectorizer.get_feature_names())\r\n",
        "# print(vectors_train_stop_Lemma_binary)\r\n",
        "print(vectors_test_stop_Lemma_binary.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1378, 9779)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymF0ztmLnbGq"
      },
      "source": [
        "## Experiments with models in sk-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH-sQOatGy54"
      },
      "source": [
        "vectors_train，vectors_test是加了onehot的，vectors_train_stop_Lemma是把几种处理都加上的，target是y_train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PccMQje5G0wq"
      },
      "source": [
        "from sklearn.naive_bayes import BernoulliNB"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMEu17OWG_qy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f82f49d-3e4b-4658-bb21-4a1504ff905a"
      },
      "source": [
        "X_train = vectors_train_stop_Lemma\n",
        "y_train = y_train\n",
        "X_test = vectors_test_stop_Lemma\n",
        "# y_test unknown\n",
        "model = BernoulliNB()\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_train, y_train)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9409704852426213"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-2Gt9mxNfyH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "015ef875-ce64-4112-e8d5-740bb4b2fd52"
      },
      "source": [
        "X_train = vectors_train\n",
        "y_train = y_train\n",
        "X_test = vectors_test\n",
        "# y_test unknown\n",
        "model = BernoulliNB()\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_train, y_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9074537268634317"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm5JthkuOxfP"
      },
      "source": [
        "## Kfold cv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym2wjVFJPyhl"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "model = LinearSVC()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjLrCRIVOwn-"
      },
      "source": [
        "X = vectors_train_stop_Lemma\n",
        "y = y_train\n",
        "# X_test = vectors_test_stop_Lemma\n",
        "# y_test unknown"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9Lb1SuWNzg4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7877a05-d9be-4977-e0fa-f69e43a2f2cb"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=10, shuffle=True)\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"test accu: \", model.score(X_test, y_test))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accu:  0.94\n",
            "test accu:  0.915\n",
            "test accu:  0.93\n",
            "test accu:  0.895\n",
            "test accu:  0.94\n",
            "test accu:  0.92\n",
            "test accu:  0.96\n",
            "test accu:  0.965\n",
            "test accu:  0.92\n",
            "test accu:  0.9246231155778895\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guvT3RTzGgxk"
      },
      "source": [
        "## Bernoulli NB model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUmRiQt-oF5e"
      },
      "source": [
        "import time\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMSndDpV918Z"
      },
      "source": [
        "class Bernoulli_NB():\r\n",
        "  def __init__(self, LaplaceSmoothing = True):\r\n",
        "    self.LaplaceSmoothing = LaplaceSmoothing\r\n",
        "    self.Prob_Y = None      # P(Y)\r\n",
        "    self.Prob_X_Y = None     # P(xj|Y)\r\n",
        "    self.Prob_X_Y = None     # P(xj|Y)\r\n",
        "    self.n_class = 0\r\n",
        "    self.w0 = None\r\n",
        "    self.w = None\r\n",
        "    self.Ytarget = None\r\n",
        "\r\n",
        "  def ProbY(self, Y):\r\n",
        "    # calculate P(Y=1) and P(Y=0)\r\n",
        "    ProbY = np.zeros((1,2))\r\n",
        "    ProbY[0,1] = np.sum(Y)/np.shape(Y)\r\n",
        "    ProbY[0,0] = 1 - ProbY[0,1]\r\n",
        "    return ProbY\r\n",
        "\r\n",
        "  def ProbX_Yi(self, X, Y, label):\r\n",
        "    # calculte P(xj=1|Yi=1)\r\n",
        "    rows,cols = X.shape       # feature shape\r\n",
        "    numerator = np.zeros((1,cols))    # initialize numerator\r\n",
        "\r\n",
        "    # xj=1 and Yi=1\r\n",
        "    for n in range(rows):\r\n",
        "      if Y[n] == label:\r\n",
        "        numerator += X[n,:]\r\n",
        "    # Yi=1\r\n",
        "    denominator = np.count_nonzero(Y == label)\r\n",
        "\r\n",
        "    # Laplace Smoothing\r\n",
        "    if(self.LaplaceSmoothing):\r\n",
        "      numerator += 1\r\n",
        "      denominator += 2\r\n",
        "\r\n",
        "    # P(xj=1|Yi=1)\r\n",
        "    prob = numerator/denominator\r\n",
        "    return prob    \r\n",
        "\r\n",
        "  def fit(self, X, Y):\r\n",
        "    print('---------------------- start fitting ---------------------')\r\n",
        "    t1 = time.time()\r\n",
        "    \r\n",
        "    rows,cols = X.shape              # feature shape\r\n",
        "    self.n_class = len(np.unique(Y))       # number of classes\r\n",
        "    self.Prob_Y = np.zeros((self.n_class,2))   # initialize P(Y)\r\n",
        "    self.Prob_X_Y = np.zeros((self.n_class,2,cols)) # initialize P(x|Y)\r\n",
        "    c = np.zeros((self.n_class,cols))       # rows:class cols:xj\r\n",
        "    d = np.zeros((self.n_class,cols))       # rows:class cols:xj\r\n",
        "    self.w0 = np.zeros((1,self.n_class))     # [w0Y1,w0Y2,...]\r\n",
        "    self.w = np.zeros((self.n_class,cols))    # [(w1,w2,...)Y1;\r\n",
        "                            # (w1,w2,...)Y2]\r\n",
        "    for Yi in range(self.n_class):\r\n",
        "      Y_onevsall = np.where(Y == Yi, 1, 0)    # only have 2 classes: Yi(1) & notYi(0)\r\n",
        "      self.Prob_Y[Yi,:] = self.ProbY(Y_onevsall)   # [P(notYi), P(Yi)]\r\n",
        "      self.Prob_X_Y[Yi,0,:] = self.ProbX_Yi(X,Y_onevsall,0)  # [P(x1|notYi), P(x2|notYi),...]\r\n",
        "      self.Prob_X_Y[Yi,1,:] = self.ProbX_Yi(X,Y_onevsall,1)  # [P(x1|Yi), P(x2|Yi),...]\r\n",
        "      c[Yi,:] = np.log10(self.Prob_X_Y[Yi,1,:]/self.Prob_X_Y[Yi,0,:])     # log(P(xj|Y=1)/P(xj|Y=0))\r\n",
        "      d[Yi,:] = np.log10((1-self.Prob_X_Y[Yi,1,:])/(1-self.Prob_X_Y[Yi,0,:])) # log((1-P(xj|Y=1))/(1-P(xj|Y=0)))\r\n",
        "      self.w0[0,Yi] = np.log10(self.Prob_Y[Yi,1]/self.Prob_Y[Yi,0]) + np.sum(d[Yi,:])\r\n",
        "      self.w[Yi,:] = c[Yi,:] - d[Yi,:]\r\n",
        "\r\n",
        "    print('------ fit done, total time: ',time.time()-t1,' -----')\r\n",
        "    # return self.Prob_Y,self.Prob_X_Y\r\n",
        "\r\n",
        "  def predict(self, X):\r\n",
        "    print('---------------------- start predict ---------------------')\r\n",
        "    t1 = time.time()\r\n",
        "    \r\n",
        "    #X = self.encoder(X)\r\n",
        "    rows,cols = np.shape(X)       # feature shape\r\n",
        "    PreY = np.zeros(rows) # initialize Y\r\n",
        "    # print('type',type(PreY),'pre',PreY)\r\n",
        "    LogOddsRatio = np.zeros((1,self.n_class))  # initialize log odds ratio a(x) \r\n",
        "    Logistic = np.zeros((1,self.n_class))    # initialize logistic function\r\n",
        "    Y_index = 0\r\n",
        "    for obs in range(rows):\r\n",
        "      for Yi in range(self.n_class): \r\n",
        "        part2 = self.w[Yi,:].reshape(1,cols) @ np.transpose(X[obs,:])\r\n",
        "        LogOddsRatio[0,Yi] = self.w0[0,Yi] + part2.astype(np.float64)\r\n",
        "        Logistic[0,Yi] = 1/(1+np.exp(-LogOddsRatio[0,Yi]))\r\n",
        "      #print('w shape',self.w[Yi,:].reshape(1,cols).shape)\r\n",
        "      #print('x shape',np.transpose(X[obs,:]).shape)\r\n",
        "      #print('w',self.w[Yi,:].reshape(1,cols))\r\n",
        "      #print('x',np.transpose(X[obs,:]))\r\n",
        "      #print('part2',np.matmul(self.w[Yi,:].reshape(1,cols),np.transpose(X[obs,:])))\r\n",
        "      #print(np.where(Logistic == np.amax(Logistic))[1])\r\n",
        "      PreY[obs] = np.where(Logistic == np.amax(Logistic))[1]\r\n",
        "    print('------ predict done, total time: ',time.time()-t1,' -----')\r\n",
        "    return PreY\r\n",
        "\r\n",
        "  def score(self,X,Y):\r\n",
        "    # Return the mean accuracy on the given test data and labels.\r\n",
        "    PreY = self.predict(X)\r\n",
        "    rows = np.shape(Y)[0]\r\n",
        "    n_correct = 0;\r\n",
        "    for obs in range(rows):\r\n",
        "      if PreY[obs] == Y[obs]:\r\n",
        "        n_correct += 1\r\n",
        "    #print('n:',n_correct)\r\n",
        "    #print('rows:',rows)\r\n",
        "    accuracy = n_correct / rows\r\n",
        "    print('------------------ accuracy:',accuracy,' -----------------')\r\n",
        "    return accuracy\r\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvXNQVB42k8Y",
        "outputId": "d281648e-6726-448e-ba00-855534029c14"
      },
      "source": [
        "vectors_train_stop_Lemma_binary.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1999, 9779)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmXi0euc2nwI",
        "outputId": "cd751e3a-434c-4cc1-811c-228e82818978"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1800,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "DfnCeH3hCxtV",
        "outputId": "c0b06f35-f809-4d26-f824-02593660d7e2"
      },
      "source": [
        "B = Bernoulli_NB()\r\n",
        "'''\r\n",
        "parameters = {\r\n",
        "...     'vect__ngram_range': [(1, 1), (1, 2)],\r\n",
        "...     'tfidf__use_idf': (True, False),\r\n",
        "... }'''\r\n",
        "\r\n",
        "B.fit(vectors_train_stop_Lemma_binary,y_train)\r\n",
        "B.score(vectors_train_stop_Lemma_binary,y_train)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------- start fitting ---------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-0124faf6640e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m ... }'''\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors_train_stop_Lemma_binary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors_train_stop_Lemma_binary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-dc9311131d5e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     54\u001b[0m       \u001b[0mY_onevsall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mYi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# only have 2 classes: Yi(1) & notYi(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProb_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mYi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProbY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_onevsall\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# [P(notYi), P(Yi)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProb_X_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mYi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProbX_Yi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_onevsall\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [P(x1|notYi), P(x2|notYi),...]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProb_X_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mYi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProbX_Yi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_onevsall\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [P(x1|Yi), P(x2|Yi),...]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m       \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mYi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProb_X_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mYi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProb_X_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mYi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# log(P(xj|Y=1)/P(xj|Y=0))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-dc9311131d5e>\u001b[0m in \u001b[0;36mProbX_Yi\u001b[0;34m(self, X, Y, label)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# xj=1 and Yi=1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mnumerator\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Yi=1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 1800 is out of bounds for axis 0 with size 1800"
          ]
        }
      ]
    }
  ]
}